{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-Chunking Data\n",
    "\n",
    "Re-organizing stored data such that it matches the analysis use-case.\n",
    "\n",
    "Inspiration from:\n",
    "<https://github.com/NCAR/rechunk_retro_nwm_v21/blob/main/notebooks/usage_example_rerechunk_chrtout.ipynb>\n",
    "\n",
    "  \n",
    ":::{dropdown} A guide to pre-requisites and learning objectives\n",
    "* Pre-Requisites &mdash; To get the most out of this notebook, you should already understand:\n",
    "  * one\n",
    "  * two\n",
    "  * three\n",
    "* Learning Objectives &mdash; At the end of this notebook, you should be able to:\n",
    "  * one\n",
    "  * two\n",
    "  * three\n",
    ":::\n",
    "\n",
    ":::{note}\n",
    "* The [`rechunker` documentation](https://rechunker.readthedocs.io/en/latest/index.html) contains several \n",
    "examples and a tutorial covering how to re-chunk data.  Much of what is here replicates concepts covered\n",
    "in that material.  This document uses data that _looks_ like `HyTest` data (variable names, extent, etc), \n",
    "so may offer a smoother intro to the concepts using familiar data.\n",
    "* The `zarr` data standard has a nice tutorial also which covers details of \n",
    "  [optimizing chunking strategies](https://zarr.readthedocs.io/en/stable/tutorial.html#changing-chunk-shapes-rechunking).\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "### What is chunking and why should you care?\n",
    "\n",
    "The idea of data '_chunks_' is closely aligned with the NetCDF and [zarr](https://zarr.dev/) standards for storing N-dimensional arrays of typed data. \n",
    "\n",
    "Chunks become more important as the size of the array increases.  For very large arrays, it is helpful to organize the memory it occupies  into sub-units.  These sub-units are the chunks -- note that this is not another dimension to the array, but merely a map to how the large array is partitioned into more palatable sized units for manipulation in memory. Array-handling libraries (numpy, xarray, pandas, and others) will handle all of the record-keeping to know which chunk holds a given unit of the array. \n",
    "\n",
    "A quick side-bar to illustrate two chunking patterns for a simple 2D array.  This is a simplified use-case.  Consider a square array of integer values. Just for exposition, let's use a small array 10x10. \n",
    "\n",
    "```\n",
    "illustration\n",
    "```\n",
    "\n",
    "That array can be organized in memory in a few ways... two common options are **row-major** order, and **column-major** order:\n",
    "* Row-Major -- A row of data occupies a contiguous block of memory. This implies that cells which are logicall adjacent vertically are not physicall near one another in memory. The 'distance' from `r0c0` to `r0c1` (a one-cell logical move within the row) is short, while the 'distance' to `r1c0` (a one-cell logical move within the column) is long.\n",
    "* Column-Major -- A column of the array occupies a contiguious block of memory. This implies that cells which are adjacent horizontally are not near one another physically in memory. \n",
    "\n",
    "In either chunk mapping, `r3c5` (for example) still fetches the same value -- the array still indexes/addresses in the same way -- but the chunking plan determines how nearby an 'adjacent' index is. \n",
    "\n",
    "As the size of the array increases, the chunk pattern becomes more relevant. Suppose your data is chunked by **rows**, and you need to process a **column** of data -- your process will need to read a lot of data, skipping most of it (possibly going to disk to read more data if the dataset is very large), to get the $i^{th}$ column value for each row. For this analysis, it would be better if the array could be '_re-chunked_' from row-major order to column-major order.  This would favor column operations.\n",
    "\n",
    "Array size is important to the chunking plan... it could be that an entire row of data in a large 2D array won't fit into memory (or it can't fit into a contiguous block of memory).  This would require chunking each row of data, in addition to chunking rows over columns (for a row-major plan) As dimensions are added to the array, this chunk-mapping becomes more complex and it becomes much more relevant to chunk the data to match the analysis. \n",
    "\n",
    "### Pros &amp; Cons\n",
    "Data that is well-organized to optimize one kind of analysis may not suit another kind of analysis on the same data. Re-chunking is time-consuming, and it produces a separate copy of the dataset, increasing storage requirements. The initial time commitment is a one-time operation so that future analyses can run quickly. The space commitment can be substantial if a complex dataset needs to be organized for many different analyses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining a Small Dataset\n",
    "Let's read a sample dataset and examine how it is chunked.  \n",
    "\n",
    "This example data is a small subset extracted from \n",
    "National Water Model Reanalysis Version 2.1.  The dataset is part of the \n",
    "[AWS Open Data Program](https://aws.amazon.com/opendata/), \n",
    "available via the S3 bucket at \n",
    "```\n",
    "s3://noaa-nwm-retrospective-2-1-zarr-pds/noaa-nwm-retrospective-2-1-zarr-pds/chrtout.zarr\n",
    "```  \n",
    "We've taken a small piece of this very large dataset for this tutorial (a random sampling of 400 stream gages for the month of July, 2000). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4080/585508255.py:3: RuntimeWarning: Failed to open Zarr store with consolidated metadata, falling back to try reading non-consolidated metadata. This is typically much slower for opening a dataset. To silence this warning, consider:\n",
      "1. Consolidating metadata in this existing store with zarr.consolidate_metadata().\n",
      "2. Explicitly setting consolidated=False, to avoid trying to read consolidate metadata, or\n",
      "3. Explicitly setting consolidated=True, to raise an error in this case instead of falling back to try reading non-consolidated metadata.\n",
      "  sampleData = xr.open_zarr(r'./TestData.zarr/')\n"
     ]
    },
    {
     "ename": "GroupNotFoundError",
     "evalue": "group not found at path ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/home/conda/users/76634b8b76503b686ed55f14ef74288ada4ffb7c19282b669f7edfb7fc8aed9d-20220909-192621-731199-163-pangeo/lib/python3.9/site-packages/xarray/backends/zarr.py:384\u001b[0m, in \u001b[0;36mZarrStore.open_group\u001b[0;34m(cls, store, mode, synchronizer, group, consolidated, consolidate_on_close, chunk_store, storage_options, append_dim, write_region, safe_chunks, stacklevel)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 384\u001b[0m     zarr_group \u001b[38;5;241m=\u001b[39m \u001b[43mzarr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_consolidated\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mopen_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m/home/conda/users/76634b8b76503b686ed55f14ef74288ada4ffb7c19282b669f7edfb7fc8aed9d-20220909-192621-731199-163-pangeo/lib/python3.9/site-packages/zarr/convenience.py:1304\u001b[0m, in \u001b[0;36mopen_consolidated\u001b[0;34m(store, metadata_key, mode, **kwargs)\u001b[0m\n\u001b[1;32m   1303\u001b[0m \u001b[38;5;66;03m# setup metadata store\u001b[39;00m\n\u001b[0;32m-> 1304\u001b[0m meta_store \u001b[38;5;241m=\u001b[39m \u001b[43mConsolidatedStoreClass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;66;03m# pass through\u001b[39;00m\n",
      "File \u001b[0;32m/home/conda/users/76634b8b76503b686ed55f14ef74288ada4ffb7c19282b669f7edfb7fc8aed9d-20220909-192621-731199-163-pangeo/lib/python3.9/site-packages/zarr/storage.py:2853\u001b[0m, in \u001b[0;36mConsolidatedMetadataStore.__init__\u001b[0;34m(self, store, metadata_key)\u001b[0m\n\u001b[1;32m   2852\u001b[0m \u001b[38;5;66;03m# retrieve consolidated metadata\u001b[39;00m\n\u001b[0;32m-> 2853\u001b[0m meta \u001b[38;5;241m=\u001b[39m json_loads(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstore\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmetadata_key\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m   2855\u001b[0m \u001b[38;5;66;03m# check format of consolidated metadata\u001b[39;00m\n",
      "File \u001b[0;32m/home/conda/users/76634b8b76503b686ed55f14ef74288ada4ffb7c19282b669f7edfb7fc8aed9d-20220909-192621-731199-163-pangeo/lib/python3.9/site-packages/zarr/storage.py:1067\u001b[0m, in \u001b[0;36mDirectoryStore.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1067\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: '.zmetadata'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mGroupNotFoundError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Reading a zarr file into an xarray DataSet:\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxarray\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxr\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m sampleData \u001b[38;5;241m=\u001b[39m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_zarr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./TestData.zarr/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m sampleData\n",
      "File \u001b[0;32m/home/conda/users/76634b8b76503b686ed55f14ef74288ada4ffb7c19282b669f7edfb7fc8aed9d-20220909-192621-731199-163-pangeo/lib/python3.9/site-packages/xarray/backends/zarr.py:789\u001b[0m, in \u001b[0;36mopen_zarr\u001b[0;34m(store, group, synchronizer, chunks, decode_cf, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, consolidated, overwrite_encoded_chunks, chunk_store, storage_options, decode_timedelta, use_cftime, **kwargs)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    777\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopen_zarr() got unexpected keyword arguments \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(kwargs\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m    778\u001b[0m     )\n\u001b[1;32m    780\u001b[0m backend_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msynchronizer\u001b[39m\u001b[38;5;124m\"\u001b[39m: synchronizer,\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconsolidated\u001b[39m\u001b[38;5;124m\"\u001b[39m: consolidated,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    786\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstacklevel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m    787\u001b[0m }\n\u001b[0;32m--> 789\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_cf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_cf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask_and_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_and_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_times\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconcat_characters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcat_characters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_coords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_coords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzarr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackend_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_timedelta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_timedelta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cftime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cftime\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m/home/conda/users/76634b8b76503b686ed55f14ef74288ada4ffb7c19282b669f7edfb7fc8aed9d-20220909-192621-731199-163-pangeo/lib/python3.9/site-packages/xarray/backends/api.py:531\u001b[0m, in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, backend_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    519\u001b[0m decoders \u001b[38;5;241m=\u001b[39m _resolve_decoders_kwargs(\n\u001b[1;32m    520\u001b[0m     decode_cf,\n\u001b[1;32m    521\u001b[0m     open_backend_dataset_parameters\u001b[38;5;241m=\u001b[39mbackend\u001b[38;5;241m.\u001b[39mopen_dataset_parameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    527\u001b[0m     decode_coords\u001b[38;5;241m=\u001b[39mdecode_coords,\n\u001b[1;32m    528\u001b[0m )\n\u001b[1;32m    530\u001b[0m overwrite_encoded_chunks \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite_encoded_chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 531\u001b[0m backend_ds \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdecoders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m ds \u001b[38;5;241m=\u001b[39m _dataset_from_backend_dataset(\n\u001b[1;32m    538\u001b[0m     backend_ds,\n\u001b[1;32m    539\u001b[0m     filename_or_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    548\u001b[0m )\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m/home/conda/users/76634b8b76503b686ed55f14ef74288ada4ffb7c19282b669f7edfb7fc8aed9d-20220909-192621-731199-163-pangeo/lib/python3.9/site-packages/xarray/backends/zarr.py:837\u001b[0m, in \u001b[0;36mZarrBackendEntrypoint.open_dataset\u001b[0;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, synchronizer, consolidated, chunk_store, storage_options, stacklevel)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen_dataset\u001b[39m(\n\u001b[1;32m    818\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    819\u001b[0m     filename_or_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    833\u001b[0m     stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m    834\u001b[0m ):\n\u001b[1;32m    836\u001b[0m     filename_or_obj \u001b[38;5;241m=\u001b[39m _normalize_path(filename_or_obj)\n\u001b[0;32m--> 837\u001b[0m     store \u001b[38;5;241m=\u001b[39m \u001b[43mZarrStore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynchronizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynchronizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconsolidated\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconsolidated\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconsolidate_on_close\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunk_store\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_store\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstacklevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstacklevel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    849\u001b[0m     store_entrypoint \u001b[38;5;241m=\u001b[39m StoreBackendEntrypoint()\n\u001b[1;32m    850\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m close_on_error(store):\n",
      "File \u001b[0;32m/home/conda/users/76634b8b76503b686ed55f14ef74288ada4ffb7c19282b669f7edfb7fc8aed9d-20220909-192621-731199-163-pangeo/lib/python3.9/site-packages/xarray/backends/zarr.py:401\u001b[0m, in \u001b[0;36mZarrStore.open_group\u001b[0;34m(cls, store, mode, synchronizer, group, consolidated, consolidate_on_close, chunk_store, storage_options, append_dim, write_region, safe_chunks, stacklevel)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    386\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    387\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to open Zarr store with consolidated metadata, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    388\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalling back to try reading non-consolidated metadata. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    399\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    400\u001b[0m         )\n\u001b[0;32m--> 401\u001b[0m         zarr_group \u001b[38;5;241m=\u001b[39m \u001b[43mzarr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mopen_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m consolidated:\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;66;03m# TODO: an option to pass the metadata_key keyword\u001b[39;00m\n\u001b[1;32m    404\u001b[0m     zarr_group \u001b[38;5;241m=\u001b[39m zarr\u001b[38;5;241m.\u001b[39mopen_consolidated(store, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mopen_kwargs)\n",
      "File \u001b[0;32m/home/conda/users/76634b8b76503b686ed55f14ef74288ada4ffb7c19282b669f7edfb7fc8aed9d-20220909-192621-731199-163-pangeo/lib/python3.9/site-packages/zarr/hierarchy.py:1347\u001b[0m, in \u001b[0;36mopen_group\u001b[0;34m(store, mode, cache_attrs, synchronizer, path, chunk_store, storage_options, zarr_version)\u001b[0m\n\u001b[1;32m   1345\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m contains_array(store, path\u001b[38;5;241m=\u001b[39mpath):\n\u001b[1;32m   1346\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m ContainsArrayError(path)\n\u001b[0;32m-> 1347\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m GroupNotFoundError(path)\n\u001b[1;32m   1349\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m   1350\u001b[0m     init_group(store, overwrite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, path\u001b[38;5;241m=\u001b[39mpath, chunk_store\u001b[38;5;241m=\u001b[39mchunk_store)\n",
      "\u001b[0;31mGroupNotFoundError\u001b[0m: group not found at path ''"
     ]
    }
   ],
   "source": [
    "# Reading a zarr file into an xarray DataSet:\n",
    "import xarray as xr\n",
    "sampleData = xr.open_zarr(r'./TestData.zarr/')\n",
    "sampleData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "PathNotFoundError",
     "evalue": "nothing found at path ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPathNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mzarr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconsolidate_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./TestData.zarr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/conda/users/76634b8b76503b686ed55f14ef74288ada4ffb7c19282b669f7edfb7fc8aed9d-20220909-192621-731199-163-pangeo/lib/python3.9/site-packages/zarr/convenience.py:1236\u001b[0m, in \u001b[0;36mconsolidate_metadata\u001b[0;34m(store, metadata_key, path)\u001b[0m\n\u001b[1;32m   1228\u001b[0m out \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1229\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzarr_consolidated_format\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1230\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1233\u001b[0m     }\n\u001b[1;32m   1234\u001b[0m }\n\u001b[1;32m   1235\u001b[0m store[metadata_key] \u001b[38;5;241m=\u001b[39m json_dumps(out)\n\u001b[0;32m-> 1236\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopen_consolidated\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/conda/users/76634b8b76503b686ed55f14ef74288ada4ffb7c19282b669f7edfb7fc8aed9d-20220909-192621-731199-163-pangeo/lib/python3.9/site-packages/zarr/convenience.py:1308\u001b[0m, in \u001b[0;36mopen_consolidated\u001b[0;34m(store, metadata_key, mode, **kwargs)\u001b[0m\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;66;03m# pass through\u001b[39;00m\n\u001b[1;32m   1307\u001b[0m chunk_store \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunk_store\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m store\n\u001b[0;32m-> 1308\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeta_store\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_store\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_store\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/conda/users/76634b8b76503b686ed55f14ef74288ada4ffb7c19282b669f7edfb7fc8aed9d-20220909-192621-731199-163-pangeo/lib/python3.9/site-packages/zarr/convenience.py:122\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(store, mode, zarr_version, path, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m open_group(_store, mode\u001b[38;5;241m=\u001b[39mmode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PathNotFoundError(path)\n",
      "\u001b[0;31mPathNotFoundError\u001b[0m: nothing found at path ''"
     ]
    }
   ],
   "source": [
    " zarr.consolidate_metadata('./TestData.zarr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/shared/users/gzt5142/hytest/dataset_preprocessing\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'.zmetadata'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mzarr\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m g \u001b[38;5;241m=\u001b[39m \u001b[43mzarr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvenience\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_consolidated\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./TestData.zarr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# read zarr metadata for named file.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(g\u001b[38;5;241m.\u001b[39mtree())\n",
      "File \u001b[0;32m/home/conda/users/76634b8b76503b686ed55f14ef74288ada4ffb7c19282b669f7edfb7fc8aed9d-20220909-192621-731199-163-pangeo/lib/python3.9/site-packages/zarr/convenience.py:1304\u001b[0m, in \u001b[0;36mopen_consolidated\u001b[0;34m(store, metadata_key, mode, **kwargs)\u001b[0m\n\u001b[1;32m   1299\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1300\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath must be provided to open a Zarr 3.x consolidated store\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1301\u001b[0m         )\n\u001b[1;32m   1303\u001b[0m \u001b[38;5;66;03m# setup metadata store\u001b[39;00m\n\u001b[0;32m-> 1304\u001b[0m meta_store \u001b[38;5;241m=\u001b[39m \u001b[43mConsolidatedStoreClass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;66;03m# pass through\u001b[39;00m\n\u001b[1;32m   1307\u001b[0m chunk_store \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunk_store\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m store\n",
      "File \u001b[0;32m/home/conda/users/76634b8b76503b686ed55f14ef74288ada4ffb7c19282b669f7edfb7fc8aed9d-20220909-192621-731199-163-pangeo/lib/python3.9/site-packages/zarr/storage.py:2853\u001b[0m, in \u001b[0;36mConsolidatedMetadataStore.__init__\u001b[0;34m(self, store, metadata_key)\u001b[0m\n\u001b[1;32m   2850\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstore \u001b[38;5;241m=\u001b[39m Store\u001b[38;5;241m.\u001b[39m_ensure_store(store)\n\u001b[1;32m   2852\u001b[0m \u001b[38;5;66;03m# retrieve consolidated metadata\u001b[39;00m\n\u001b[0;32m-> 2853\u001b[0m meta \u001b[38;5;241m=\u001b[39m json_loads(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstore\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmetadata_key\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m   2855\u001b[0m \u001b[38;5;66;03m# check format of consolidated metadata\u001b[39;00m\n\u001b[1;32m   2856\u001b[0m consolidated_format \u001b[38;5;241m=\u001b[39m meta\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzarr_consolidated_format\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/home/conda/users/76634b8b76503b686ed55f14ef74288ada4ffb7c19282b669f7edfb7fc8aed9d-20220909-192621-731199-163-pangeo/lib/python3.9/site-packages/zarr/storage.py:1067\u001b[0m, in \u001b[0;36mDirectoryStore.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1065\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fromfile(filepath)\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1067\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: '.zmetadata'"
     ]
    }
   ],
   "source": [
    "import zarr\n",
    "g = zarr.convenience.open_consolidated('./TestData.zarr') # read zarr metadata for named file.\n",
    "print(g.tree())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The critical items to notice in this output are highlighted here: \n",
    "<pre>\n",
    "&lt;xarray.Dataset&gt;\n",
    "\n",
    "Dimensions:     (feature_id: 400, time: 744) <span style=\"color:red\"> &lt;-- NOTE: Two dimensions</span>\n",
    "\n",
    "                 <span style=\"color:red\">  +--- most coordinates are tied to feature_id dimension</span> \n",
    "                 <span style=\"color:red\">  |</span> \n",
    "Coordinates:     <span style=\"color:red\">  V</span>\n",
    "    elevation   (feature_id) float32 dask.array&lt;chunksize=(400,), meta=np.ndarray&gt;\n",
    "  * feature_id  (feature_id) int32 3109 189899 239166 ... 947070134 1010003783\n",
    "    gage_id     (feature_id) |S15 dask.array&lt;chunksize=(400,), meta=np.ndarray&gt;\n",
    "    latitude    (feature_id) float32 dask.array&lt;chunksize=(400,), meta=np.ndarray&gt;\n",
    "    longitude   (feature_id) float32 dask.array&lt;chunksize=(400,), meta=np.ndarray&gt;\n",
    "    order       (feature_id) int32 dask.array&lt;chunksize=(400,), meta=np.ndarray&gt;\n",
    "  * time        (time) datetime64[ns] 2000-07-01 ... 2000-07-31T23:00:00\n",
    "\n",
    "Data variables:\n",
    "    streamflow  (<span style=\"color:green\">time, feature_id</span>) float64 dask.array&lt;<span style=\"color:green\">chunksize=(256, 16)</span>, meta=np.ndarray&gt;\n",
    "    velocity    (time, feature_id) float64 dask.array&lt;chunksize=(256, 16), meta=np.ndarray&gt;\n",
    "                 <span style=\"color:red\">^^^^  ^^^^^^^^^^</span>\n",
    "                 <span style=\"color:red\">the data variables are addressed by both dimensions; this is 2D data.</span>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is a 'stack' of two 2D arrays.  They are named 'streamflow' and 'velocity'.  The indices \n",
    "into each of those 2D arrays are `time` on one axis, and `feature_id` on the other.  The feature id \n",
    "is bound to a number of other coordinates, so you can relate/refer to a given feature by its elevation, \n",
    "gage_id, latitude, longitude, or stream order. \n",
    "\n",
    "Note the `chunksize` highlighted in green. This says that the data is stored in blocks mapping to 256 \n",
    "adjacent time-steps for 16 adjacent features. (**NOTE**: _The original data is not chunked this way; we've \n",
    "deliberately fiddled with the chunk configuration for this tutorial_)\n",
    "\n",
    "A time-series analysis (i.e. sampling all time-step values for a single `feature_id`) would require \n",
    "multiple chunks to be fetched. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch all the time values for a specific feature_id\n",
    "sampleData['streamflow'].sel(feature_id=1343034)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data has 744 time-steps available, chunked into chunks of 256 values each. Three chunks are needed to hold this time-series for one feature.  Not too bad, but not good either. \n",
    "\n",
    "On the other hand, an analysis which samples all locations for a single point in time would need \n",
    "to fetch multiple chunks also. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch all the gage values for a single day\n",
    "sampleData['streamflow'].sel(time='07-01-2000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This dataset has 400 features, broken into chunks of 16 data values each. Many more chunks to fetch.  This is much worse: the I/O engine needs to find and retrieve 25 chunks vs 3 in the previous example. \n",
    "\n",
    "If we were going to do either of those analyses on a very large dataset with this pattern, we'd want to \n",
    "re-chunk the data to optimize for our read pattern.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-Chunking the Sample Data\n",
    "This is a trivial example, due to the small size of the dataset -- It all fits in memory easily. But it is worth doing, as concepts will apply when we take this to the full-sized data.\n",
    "\n",
    "First thing we need is a chunk plan to describe the chunk layout we want. This can be generated using various methods.  For this dataset, it's easy enough to write it manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numbers are *size* of the chunk. \n",
    "chunk_plan = {\n",
    "    'streamflow': {'time': 744, 'feature_id': 1}, # all time records in one chunk for each feature_id\n",
    "    'velocity': {'time': 744, 'feature_id': 1},\n",
    "    'elevation': (400,),\n",
    "    'gage_id': (400,),\n",
    "    'latitude': (400,),\n",
    "    'longitude': (400,),    \n",
    "    'order': (400,),    \n",
    "    'time': (744,),\n",
    "    'feature_id': (400,)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this plan, we can ask `rechunker` to re-write the data using the prescribed chunking pattern.\n",
    "\n",
    "Because this will be a large dataset meant for consumption via cloud computing, we will write it to a\n",
    "S3 bucket.  We need to set up that system before asking `rechunker` to start writing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_s3(url):\n",
    "    fs1 = fsspec.open(url, anon=False).fs\n",
    "    if fs1.exists(url):\n",
    "        fs1.rm(url, recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fsspec\n",
    "os.environ['AWS_PROFILE'] = 'nhgf-development'\n",
    "\n",
    "fs = fsspec.filesystem('s3', anon=False, skip_instance_cache=False)\n",
    "workspace = 's3://nhgf-development/workspace/'\n",
    "myDir = workspace + 'testing/gzt5142/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.mkdir(myDir, create_parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in ['rechunked.zarr', 'staging.zarr']:\n",
    "    if fs.exists(myDir + f):\n",
    "        fs.rm(myDir + f, recursive=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "staging = fs.get_mapper(myDir + 'staging.zarr')\n",
    "outfile = fs.get_mapper(myDir + 'rechunked.zarr')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rechunker\n",
    "result = rechunker.rechunk(\n",
    "    sampleData,\n",
    "    chunk_plan,\n",
    "    \"2GB\",                #<--- Max Memory\n",
    "    outfile, \n",
    "    temp_store=staging \n",
    ")\n",
    "_ = result.execute() # Note that we must specifically direct rechunk to calculate.\n",
    "# without the call to execute(), the zarr dataset will be empty, and result will hold only\n",
    "# a 'task graph' outlining the calculation steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `rechunker.rechunk` does not overwrite any data.  If it sees that `/tmp/outfile.zarr` or `/tmp/scratch.zarr` already exist, it will balk and likely raise an exception. Be sure that these locations do not exist. \n",
    "\n",
    "The `rechunker` also writes a minimalist data group.  Meaning that variable metadata is not consolidated. This is not a required step, but it will really spead up future workflows when the\n",
    "data is read back in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "_ = zarr.consolidate_metadata(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "Let's read in the resulting re-chunked dataset to see how it looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reChunkedData = xr.open_zarr(outfile)\n",
    "reChunkedData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that for both `streamflow` and `velocity`, the chunksize in the `time` dimension is 744 (the total number of time steps). Analyses which favor fetching all time-step values for a given `facility_id` will prefer this chunking strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Before:\n",
    "sampleData['streamflow'].sel(feature_id=1343034)\n",
    "# Note: three chunks needed to service a single feature_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## After:\n",
    "reChunkedData['streamflow'].sel(feature_id=1343034) \n",
    "# All data for the specified feature_id is in a single chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "users-pangeo",
   "language": "python",
   "name": "conda-env-users-pangeo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "4100cc85ffefb381c538d28dd18cb927e5a99f05bbed6aaad5313d7bb1c2079e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
