{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing CONUS404 and reference data\n",
    "\n",
    "Short paragraph describing what is about to happen\n",
    "\n",
    "<details>\n",
    "  <summary>Guide to pre-requisites and learning outcomes...&lt;click to expand&gt;</summary>\n",
    "  \n",
    "  <table>\n",
    "    <tr>\n",
    "      <td>Pre-Requisites\n",
    "      <td>To get the most out of this notebook, you should already have an understanding of these topics: \n",
    "        <ul>\n",
    "        <li>pre-req one\n",
    "        <li>pre-req two\n",
    "        </ul>\n",
    "    <tr>\n",
    "      <td>Expected Results\n",
    "      <td>At the end of this notebook, you should be able to: \n",
    "        <ul>\n",
    "        <li>outcome one\n",
    "        <li>outcome two\n",
    "        </ul>\n",
    "  </table>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library imports\n",
    "import fsspec\n",
    "import hvplot.xarray\n",
    "import intake\n",
    "import os\n",
    "import warnings\n",
    "import requests\n",
    "import rioxarray\n",
    "import dask\n",
    "import metpy\n",
    "\n",
    "from shapely.geometry import Polygon\n",
    "from dask.distributed import LocalCluster, Client\n",
    "from pygeohydro import pygeohydro\n",
    "from fsspec.implementations.ftp import FTPFileSystem\n",
    "\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import geoviews as gv\n",
    "import dask.dataframe as dd\n",
    "import cartopy.crs as ccrs #testing\n",
    "# add polyedit, polydraw\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving data from HPC or the Cloud\n",
    "#### The process varies based on where the notebook is being run but generally looks this:\n",
    "1. (Done already) Connect to workspace (local, HPC, or QHUB) and open notebook\n",
    "2. Start Dask client \n",
    "3. Pull in data from source\n",
    "4. Process the data into usable file format, size, and extent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update tp helper function after repo consolidation\n",
    "## **Start a Dask client using an appropriate Dask Cluster** \n",
    "This is an optional step, but can speed up data loading significantly, especially when accessing data from the Cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_cluster(machine):\n",
    "    ''' Helper function to configure cluster\n",
    "    '''\n",
    "    if machine == 'denali':\n",
    "        from dask.distributed import LocalCluster, Client\n",
    "        cluster = LocalCluster(threads_per_worker=1)\n",
    "        client = Client(cluster)\n",
    "    \n",
    "    elif machine == 'tallgrass':\n",
    "        from dask.distributed import Client\n",
    "        from dask_jobqueue import SLURMCluster\n",
    "        cluster = SLURMCluster(queue='cpu', cores=1, interface='ib0',\n",
    "                               job_extra=['--nodes=1', '--ntasks-per-node=1', '--cpus-per-task=1'],\n",
    "                               memory='6GB')\n",
    "        cluster.adapt(maximum_jobs=30)\n",
    "        client = Client(cluster)\n",
    "        \n",
    "    elif machine == 'local':\n",
    "        import os\n",
    "        import warnings\n",
    "        from dask.distributed import LocalCluster, Client\n",
    "        warnings.warn(\"Running locally can result in costly data transfers!\\n\")\n",
    "        n_cores = os.cpu_count() # set to match your machine\n",
    "        cluster = LocalCluster(threads_per_worker=n_cores)\n",
    "        client = Client(cluster)\n",
    "        \n",
    "    elif machine in ['esip-qhub-gateway-v0.4']:   \n",
    "        import sys, os\n",
    "        sys.path.append(os.path.join(os.environ['HOME'],'shared','users','lib'))\n",
    "        import ebdpy as ebd\n",
    "        aws_profile = 'esip-qhub'\n",
    "        ebd.set_credentials(profile=aws_profile)\n",
    "\n",
    "        aws_region = 'us-west-2'\n",
    "        endpoint = f's3.{aws_region}.amazonaws.com'\n",
    "        ebd.set_credentials(profile=aws_profile, region=aws_region, endpoint=endpoint)\n",
    "        worker_max = 30\n",
    "        client,cluster = ebd.start_dask_cluster(profile=aws_profile, worker_max=worker_max, \n",
    "                                              region=aws_region, use_existing_cluster=True,\n",
    "                                              adaptive_scaling=False, wait_for_cluster=False, \n",
    "                                              worker_profile='Medium Worker', propagate_env=True)\n",
    "        \n",
    "    return client, cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select workspace from the following list:\n",
    "'denali', 'tallgrass', 'local', or 'esip-qhub-gateway-v0.4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View available datasets from the Intake Catalog and choose which to use\n",
    "Note: Select datasets that end in \"onprem\" if running on Denali/Tallgrass HPC or cloud data if working on QHub or local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/hytest-org/hytest/main/dataset_catalog/hytest_intake_catalog.yml'\n",
    "cat = intake.open_catalog(url)\n",
    "list(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can setup your client and dataset on QHub like this:\n",
    "Uncomment single commented spaces (#) to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set machine\n",
    "machine = 'esip-qhub-gateway-v0.4'\n",
    "\n",
    "# use configure cluster helper function to setup dask\n",
    "client, cluster = configure_cluster(machine)\n",
    "\n",
    "# set dataset\n",
    "dataset = 'conus404-hourly-cloud'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Or if on the HPC you can setup your client and dataset like this:\n",
    "Uncomment single commented spaces (#) to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set machine\n",
    "# machine = os.environ['SLURM_CLUSTER_NAME']\n",
    "\n",
    "## use configure_cluster helper function to setup dask\n",
    "# client, cluster = configure_cluster(machine)\n",
    "\n",
    "## set dataset\n",
    "# dataset = 'conus404-hourly-onprem'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Retrieve CONUS404 from source and tranform it to a Dask array**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# double check that dataset is in catalog (cat)\n",
    "dataset = 'conus404-hourly-cloud'\n",
    "cat[dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform data to dask array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = cat[dataset].to_dask()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View dataset metadata. To view variables, expand the \"Data variables\" section. \n",
    "\n",
    "For this tutorial, we will be working with accumulated precipitation (PREC_ACC_NC), air temperature (TK), and surface net radiation (RNET) variables. RNET req "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Explore the variable** \n",
    "(sometimes called exploratory data analysis (EDA) or exploratory spatial data analysis (ESDA) when it contains cartographic data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets look at the accumulated precipitation variable by first subsetting the larger dataset. \n",
    "Notice the information in the array and chunk columns as well as the coordinates (in particular *time*) and the units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable PREC_ACC_NC\n",
    "prec = ds.PREC_ACC_NC\n",
    "prec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, lets visualize a map of the data at specific time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec_time = prec.sel(time='2014-03-01 00:00').load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous cell, the .sel() method filters the dataset by the *time* coordinate through \"time=\" and then uses the .load() method to load the dataset into memory.\n",
    "\n",
    "Now, let's visualize the dataset using the [QuadMesh](https://holoviews.org/reference/elements/bokeh/QuadMesh.html) plot from Holoviews. For a more in-depth tutorial for visualizing gridded data in Holoviews, go to [Gridded Datasets](http://holoviews.org/getting_started/Gridded_Datasets.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec_time.hvplot.quadmesh(x='lon', y='lat', rasterize=True, geo=True, tiles='OSM', alpha=0.7, cmap='turbo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can also look at a time-series for a specific grid cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec_point = prec.isel(y=600,x=600).sel(time=slice('2015-02-11 00:00','2015-04-28 00:00')).load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the previous cell uses the .isel() method, which returns the dataset from where the **x** and **y** indexes equal 600 prior to filtering by **time** and loading the data into memory.\n",
    "\n",
    "Lets plot the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec_point.hvplot(x='time', grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing geographic extents\n",
    "Sometimes the data that is brought in is not analyzed but is used to clip a larger dataset to an area of interest (AOI).  <br>\n",
    "Let's look at two ways this can be done: a user-defined polygon or using the pyNHD package. Data can also be brought in other ways such as a local file or an API request. These are covered in other tutorials. <br>\n",
    "We'll show how to use geometries to limit datasets later in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The first method will use the the Holoviews and Geoviews libraries to let a user draw and then add the polygon dimensions to a geopandas GeoDataFrame.\n",
    "\n",
    "When the next code block is run, a map will open and the PolyDraw tool automatically selected. Double tap to add the first vertex, then use tap to add each subsequent vertex, to finalize the draw action double tap to insert the final vertex or press the ESC key to stop drawing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use CartoLight basemap\n",
    "basemap = gv.tile_sources.CartoLight()\n",
    "\n",
    "# x and y limits for CONUS\n",
    "xlim = (-135, -50)\n",
    "ylim = (22, 50)\n",
    "\n",
    "#create blank polygon to draw\n",
    "## redim.range works with Bokeh backend to set default map extent\n",
    "blank_poly = gv.Polygons([]).redim.range(Longitude=xlim, Latitude=ylim)\n",
    "\n",
    "# set PolyDraw for creation and PolyEdit for editing polygon, num_objects keeps to single object at a time\n",
    "user_poly = PolyDraw(source=blank_poly, show_vertices=True, num_objects=1)\n",
    "user_poly_edit = PolyEdit(source=blank_poly)\n",
    "\n",
    "# create plots\n",
    "## active_tools set to allow instant polygon drawing\n",
    "basemap.options(width=700, height=400) * blank_poly.options(\n",
    "    active_tools=['poly_draw'], fill_alpha=0.2, line_color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code block pulls the latitude and longitude coordinates for the polygon vertices that were just drawn and creates a polygon GeoDataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract lists of lat/long coordinates\n",
    "long = user_poly.data['xs'][0]\n",
    "lat = user_poly.data['ys'][0]\n",
    "\n",
    "# create zip of polygon vertices\n",
    "vertices = zip(long, lat)\n",
    "\n",
    "# construct polygon in GDF\n",
    "polygon = gpd.GeoDataFrame(\n",
    "    index=[0], crs=\"EPSG:4326\", geometry=[Polygon(vertices)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot polygon to confirm the shape matches what was drawn\n",
    "polygon.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The second method will be importing the HUC6 boundaries using the PyGeoHydro library. PyGeoHydro is a part of the HyRiver library and is document [here](https://docs.hyriver.io/autoapi/pygeohydro/index.html).\n",
    "\n",
    "The following cell queries the Water Boundary Dataset HUC6 layer and returns a GeoDataFrame from the .byids() function by examing the \"huc6\" field for the list of HUC6 id's. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drb = pygeohydro.WBD(\"huc6\", outfields=[\"huc6\", \"name\"]).byids(\"huc6\", [\"020401\", \"020402\"])\n",
    "drb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see there are two polygons in the GeoDataFrame and ploting it them confirms this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drb.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you want to use geometries to refine datasets to an AOI, it is best to have a single, concise geometry. We'll combine them in the next code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a column where all entries have the same value\n",
    "drb[\"name\"] = \"DRB\"\n",
    "\n",
    "#dissolve by that column\n",
    "drb = drb.dissolve(by=\"name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to make sure it worked by examing the tabular and spatial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tabular \n",
    "drb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial\n",
    "drb.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Putting it together: Preprocess CONUS404 to variable and research spatial extent**\n",
    "In this section we are going to put together some skills we have learned so far: bring in CONUS404, select our variables and time extent, then clip to our spatial extent.\n",
    "\n",
    "Variables: Accumulated precipitation (PREC_ACC_NC), air temperature (TK), and surface net radiation (RNET) <br>\n",
    "Time period: 01/01/1990 - 12/31/1999 <br>\n",
    "Spatial extent: Delaware River Basin <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up conus404 filename\n",
    "conus404 = 'conus404-hourly-cloud'\n",
    "\n",
    "# create dask array from dataset\n",
    "ds = cat[conus404].to_dask()\n",
    "ds = ds.metpy.parse_cf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get CRS from CONUS404 dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crs = ds.TK.metpy.cartopy_crs\n",
    "# crs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cell below caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring in boundaries of DRB and create single polygon\n",
    "drb = pygeohydro.WBD(\"huc6\", outfields=[\"huc6\", \"name\"]).byids(\"huc6\", [\"020401\", \"020402\"])\n",
    "# create a column where all entries have the same value\n",
    "drb[\"name\"] = \"DRB\"\n",
    "\n",
    "# dissolve by that column\n",
    "drb = drb.dissolve(by=\"name\")\n",
    "\n",
    "# set CRS to match ds\n",
    "drb = drb.iloc[[0]].to_crs(crs)\n",
    "\n",
    "#visualize\n",
    "# drb.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#subset data variables\n",
    "c404_variables = [\"PREC_ACC_NC\", \"TK\", \"ACSWDNB\", \"I_ACSWDNB\", \"ACSWDNB\", \"I_ACSWDNB\", \"ACLWDNB\", \"I_ACLWDNB\", \"ACLWUPB\", \"I_ACLWUPB\"]\n",
    "c404 = ds[c404_variables]\n",
    "\n",
    "# write CRS\n",
    "c404.rio.write_crs(crs, inplace=True)\n",
    "\n",
    "# perform clip\n",
    "c404_drb = c404.rio.clip(drb.geometry, crs=crs, drop=True, invert=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_drb[\"ACSWDNB\"].isel(time=-1).hvplot(x='x', y='y', crs=crs, rasterize=True, cmap='turbo', tiles='OSM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a little more processing to do before the dataset is ready for analysis. We need to:\n",
    "1. Calcuate RNET using the radiation columns\n",
    "2. Resample and aggregate the data to the desired time-step (30 days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNET is calculated using the equation <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets break down the components. First, lets tackle SWDN, which is calculated like this: <br>\n",
    "(ACSWDNB[i]+(1e9xI_ACSWDNB[i])) - (ACSWDNB[i-1]+(1e9xI_ACSWDNB[i-1])) / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"ACSWDNB\", \"I_ACSWDNB\", \"ACSWDNB\", \"I_ACSWDNB\", \"ACLWDNB\", \"I_ACLWDNB\", \"ACLWUPB\", \"I_ACLWUPB\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want all values for ACSWDNB and I_ACSWDNB starting at index = 1...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACSWDNB = c404_drb[\"ACSWDNB\"][1:]\n",
    "I_ACSWDNB = c404_drb[\"I_ACSWDNB\"][1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## checking if clip is issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds.ACLWDNB.sel(time=\"2000-01-01 12:00\").load().min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "c404.ACLWDNB.sel(time=\"2000-01-01 12:00\").load().min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "c404_drb.ACLWDNB.sel(time=\"2000-01-01 12:00\").load().min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ACSWDNB.sel(time=\"2000-01-01 12:00\").load().min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds.I_ACLWDNB.sel(time=\"2000-01-01 12:00\").load().min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "c404.I_ACLWDNB.sel(time=\"2000-01-01 12:00\").load().min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "c404_drb.I_ACLWDNB.sel(time=\"2000-01-01 12:00\").load().min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "I_ACLWDNB.sel(time=\"2000-01-01 12:00\").load().min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we want all values for ACSWDNB before index = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACSWDNB1 = c404_drb[\"ACSWDNB\"][:-1]\n",
    "ACSWDNB1.coords[\"time\"] = ACSWDNB.coords[\"time\"]\n",
    "\n",
    "I_ACSWDNB1 = c404_drb[\"I_ACSWDNB\"][:-1]\n",
    "I_ACSWDNB1.coords[\"time\"] = I_ACSWDNB.coords[\"time\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm both time coords are the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ACSWDNB.coords[\"time\"].values) == len(ACSWDNB1.coords[\"time\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ACSWDNB.coords[\"time\"].values), len(ACSWDNB1.coords[\"time\"].values), len(ds.ACSWDNB.coords[\"time\"].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing if issue is 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_ACSWDNBe9 = 1e9*I_ACSWDNB\n",
    "I_ACSWDNB1e9 = 1e9*I_ACSWDNB1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(I_ACSWDNB.sel(time=\"2000-01-01 12:00\").load().mean(), I_ACSWDNBe9.sel(time=\"2000-01-01 12:00\").load().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(I_ACSWDNB.sel(time=\"2000-01-01 12:00\").load().mean(), I_ACSWDNBe9.sel(time=\"2000-01-01 12:00\").load().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(I_ACSWDNB1.sel(time=\"2000-01-01 12:00\").load().mean(), I_ACSWDNB1e9.sel(time=\"2000-01-01 12:00\").load().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(ACSWDNB.sel(time=\"2000-01-01 12:00\").load().max(), I_ACSWDNBe9.sel(time=\"2000-01-01 12:00\").load().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addACSWDN = ACSWDNB + I_ACSWDNBe9\n",
    "# addACSWDN.sel(time=\"2000-01-01 12:00\").load().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "addACSWDNmap = addACSWDN.load()\n",
    "addACSWDNmap.hvplot(x=\"x\", y=\"y\", rasterize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addACSWDN1 = ACSWDNB1 + I_ACSWDNB1e9\n",
    "addACSWDN1.sel(time=\"2000-01-01 12:00\").load().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closeSWDN = addACSWDN - addACSWDN1\n",
    "closeSWDN.sel(time=\"2000-01-01 12:00\").load().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test not using I_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ACSWDNB.sel(time=\"2000-01-01 12:00\").load()\n",
    "test.hvplot(x='x', y='y', crs=crs, rasterize=True, cmap='turbo', tiles='OSM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ACSWDNB1.sel(time=\"2000-01-01 15:00\").load()\n",
    "test.hvplot(x='x', y='y', crs=crs, rasterize=True, cmap='turbo', tiles='OSM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with xr.set_options(arithmetic_join=\"outer\"):\n",
    "    SWDN = ACSWDNB - ACSWDNB1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SWDN = ((ACSWDNB + I_ACSWDNBe9) - (ACSWDNB1 + I_ACSWDNB1e9)) / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = SWDN.sel(time=\"2000-06-01 02:00\").load()\n",
    "test.hvplot(x='x', y='y', crs=crs, rasterize=True, cmap='turbo', tiles='OSM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = SWDN.sel(time=\"2000-06-01 06:00\").load()\n",
    "test.hvplot(x='x', y='y', crs=crs, rasterize=True, cmap='turbo', tiles='OSM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = SWDN.sel(time=\"2000-06-01 10:00\").load()\n",
    "test.hvplot(x='x', y='y', crs=crs, rasterize=True, cmap='turbo', tiles='OSM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = SWDN.sel(time=\"2000-06-01 23:00\").load()\n",
    "test.hvplot(x='x', y='y', crs=crs, rasterize=True, cmap='turbo', tiles='OSM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculate SWDN <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with xr.set_options(arithmetic_join=\"outer\"):\n",
    "    SWDN = ((ACSWDNB + I_ACSWDNBe9) - (ACSWDNB1 + I_ACSWDNB1e9)) / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SWDN.sel(time=\"2000-01-01 12:00\").load().max() #continue to see data issue above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, pad a NaN to the beginning to match original datasets time dimsension length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess reference data\n",
    "\n",
    "Now that the CONUS404 dataset has been preprocessed, it is also import for analysis to do the same with the reference data used in the comparitive analysis. In this section, data will be brought in from several sources and preprocessed in data type appropriate ways.\n",
    "\n",
    "#### NOAA's Global Historical Climate Network - Daily (GHCN) Dataset\n",
    "It is always important to review and readme or metadata files for the data you wish to bring in. The [GHCN readme](https://noaa-ghcn-pds.s3.amazonaws.com/readme.txt) is useful because it explains what is in the S3 bucket, the various columns in the datasets, and other information. When we later call in the observational data, the [by station readme](https://noaa-ghcn-pds.s3.amazonaws.com/readme-by_station.txt) provides a more detailed explanation of the data there.\n",
    "\n",
    "After reading the metadata for the file, it can be seen that only the first three columns are needed to map the stations: the station ID, latitude, and longitude. However, we want to make sure that we are only using HCN stations so we need to also use the HCN/CRN Flag column to filter to HCN sites. \n",
    "\n",
    "Start by getting a list of stations from the AWS S3 bucket where the daily data is housed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghcn_all = pd.read_csv('s3://noaa-ghcn-pds/ghcnd-stations.txt', sep=\"\\t\", header=None)\n",
    "# ghcn_all.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the file reads in as all the contents from one line ending up in one column. \n",
    "\n",
    "So we have to split the column into other columns and retain only the needed columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghcn_all = ghcn_all[0].str.split(\" +\",expand = True)\n",
    "# ghcn_all.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, columns 0-3 look as we'd expect. However, column 4 is where it starts to get messy as the method for expanding the columns has split up the station names at the spaces between. This means that the HCN flag, which we would expect to be in column 6, could be in columns 6-13. Thankfully, the pandas ```loc``` function makes do this filtering easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ushcn = ghcn_all.loc[(ghcn_all[6] == \"HCN\") | (ghcn_all[7] == \"HCN\") | (ghcn_all[8] == \"HCN\") | (ghcn_all[9] == \"HCN\") | (ghcn_all[10] == \"HCN\") | (ghcn_all[11] == \"HCN\") | (ghcn_all[12] == \"HCN\") | (ghcn_all[13] == \"HCN\")].copy()\n",
    "ushcn = ushcn.iloc[:, 0:3].rename({0:\"station\", 1:\"lat\", 2:\"lon\"}, axis=1).copy() # after the search, trim the columns and rename to get the data to what is needed to map\n",
    "# ushcn.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to clip the points to only those in the DRB. We do that by using the latitude and longitude to create a GeoDataFrame..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ushcn_gdf = gpd.GeoDataFrame(ushcn, geometry=gpd.points_from_xy(ushcn['lon'], ushcn['lat'], crs=\"EPSG:4326\"))\n",
    "\n",
    "# convert to same crs as drb\n",
    "ushcn_gdf = ushcn_gdf.to_crs(crs)\n",
    "\n",
    "# ushcn_gdf.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "followed by clipping using the *drb* geodataframe above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "drb_hcn = gpd.clip(ushcn_gdf, drb)\n",
    "# drb_hcn.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to pull in the tabular data for all of the DRB stations. These are stored on AWS in an individual CSV for each station named *station.csv*. So, we need to get all of the station IDs from our dataset and use them to create a list of URLs for these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "drb_hcn_data_url = [f\"s3://noaa-ghcn-pds/csv/by_station/{station}.csv\" for station in drb_hcn[\"station\"].unique().tolist()]\n",
    "# print(drb_hcn_data_url[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(drb_hcn_data_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now pass that list of URLs to *dask.dataframe.read_csv*, which will read the data in parallel. We'll then refine the entries to those in 1979 and after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "drb_hcn_data = dd.read_csv(drb_hcn_data_url, parse_dates=[\"DATE\"], usecols=[\"ID\", \"DATE\", \"ELEMENT\", \"DATA_VALUE\"])\n",
    "\n",
    "drb_hcn_data = drb_hcn_data.loc[drb_hcn_data[\"DATE\"] >= \"1979-01-01\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll refine the dataframe by a list of elements and then compute it.\n",
    "\n",
    "##### Note: We are using TMAX and TMIN rather than TAVG as TAVG has no records prior to 1998."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of elements we are interested in\n",
    "element_list = [\"PRCP\", \"TMAX\", \"TMIN\"]\n",
    "\n",
    "drb_hcn_data = drb_hcn_data.loc[drb_hcn_data[\"ELEMENT\"].isin(element_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check shape\n",
    "drb_hcn_data.compute().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how much memory does it take up?\n",
    "drb_hcn_data.compute().memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dask dataframe is about 21 mb in size.\n",
    "\n",
    "Similar to the CONUS404 data, we have a little more engineering to do with the data. We need to calculate the average temperatue using TMIN and TMAX (in Kelvin) as well as resample the data to a 1 month interval. We'll convert the Dask Dataframe into a Pandas Dataframe to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drb_hcn_data_df = drb_hcn_data.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by whittling down to our two elements, dropping the *ELEMENT* column, and grouping our data by *ID* and *DATE* in order to take the mean of *TMIN* and *TMAX* and convert this to degrees Kelvin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# paring down data\n",
    "drb_hcn_tk = drb_hcn_data_df.loc[drb_hcn_data_df[\"ELEMENT\"].isin([\"TMAX\",\"TMIN\"])]\n",
    "\n",
    "# dropping ELEMENT\n",
    "drb_hcn_tk = drb_hcn_tk.drop(\"ELEMENT\", axis=1)\n",
    "\n",
    "# calculate mean temperature for each station and date\n",
    "drb_hcn_tk = drb_hcn_tk.groupby([\"ID\", \"DATE\"]).mean()\n",
    "\n",
    "# rename the DATA_VALUE column to TK\n",
    "drb_hcn_tk.rename({\"DATA_VALUE\":\"TK\"}, axis=1)\n",
    "\n",
    "# convert from tenths of degrees Celsius to degrees Kelvin\n",
    "drb_hcn_tk[\"TK\"] = (drb_hcn_tk[\"TK\"] * 0.1) + 273.15\n",
    "\n",
    "# reset the index\n",
    "drb_hcn_tk.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOAA's Global Climate Reference Network (GCRN) Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use *fsspec* to make FTP call to NOAA for CRN data <br>\n",
    "First, create file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = FTPFileSystem(\"ftp.ncei.noaa.gov\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the file type is a *tsv*, we will use the *pd.read_table* function to create a Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crn_stations = pd.read_table(fs.open(\"/pub/data/uscrn/products/stations.tsv\")) \n",
    "crn_stations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now turn into GDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crn_stat = gpd.GeoDataFrame(crn_stations, geometry=gpd.points_from_xy(crn_stations[\"LONGITUDE\"], crn_stations[\"LATITUDE\"]), crs=\"EPSG:4326\")\n",
    "\n",
    "# convert to same crs as drb\n",
    "crn_stat = crn_stat.to_crs(crs)\n",
    "\n",
    "crn_stat.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find which USCRN sites are in DRB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "drb_crn = gpd.clip(crn_stat, drb)\n",
    "drb_crn.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drb_crn.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now know what CRN sites are in the Delaware River Basin. We must now retrieve the data for this site from the FTP server.\n",
    "\n",
    "First, we'll get the location name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crn_stat_name = drb_crn[\"LOCATION\"].values.tolist()[0]\n",
    "print(crn_stat_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initilize the FTP connection again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = FTPFileSystem(\"ftp.ncei.noaa.gov\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list_glob = fs.glob(\n",
    "    f\"/pub/data/uscrn/products/daily01/**/*{crn_stat_name}*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crn_data = pd.DataFrame()\n",
    "\n",
    "for file in file_list_glob:\n",
    "    stat_data = pd.read_csv(fs.open(file), header=None, sep=\"\\t\")\n",
    "    crn_data = pd.concat([crn_data, stat_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crn_data = crn_data[0].str.split(\" +\",expand = True)\n",
    "crn_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now bring in the headers for the station data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crn_headers = fs.open(\"/pub/data/uscrn/products/daily01/headers.txt\")\n",
    "crn_data_headers = pd.read_csv(crn_headers, sep=\"\\t\", header=None).iloc[1,:].str.split(\" +\").values.tolist()[0][0:28]\n",
    "crn_data_headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the number of headers equals the number of columns in our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(crn_data.columns) == len(crn_data_headers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now then rename the column headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crn_data.columns = crn_data_headers\n",
    "crn_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the *LST_DATE* column to datetime and refine columns to those of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crn_data[\"DATE\"] = pd.to_datetime(crn_data[\"LST_DATE\"])\n",
    "crn_data = crn_data[[\"DATE\", \"P_DAILY_CALC\", \"T_DAILY_MAX\", \"T_DAILY_MIN\", \"SOLARAD_DAILY\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crn_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you examine the data types, you'll see that the 4 columns of numbers are actually data type *object*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crn_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets rectify that by applying the `pd.to_numeric` function to the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = crn_data.columns.drop(\"DATE\")\n",
    "crn_data[cols] = crn_data[cols].apply(pd.to_numeric, errors='coerce')\n",
    "crn_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PRISM data\n",
    "This time we will open the PRISM dataset, clip it, and refine the data. Many of the steps will look the same as the CONUS404 dataset so there will be less explanation of the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = fsspec.filesystem(\"s3\", anon=False, requester_pays=True, skip_instance_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prism_years = range(1970, 2021, 1)\n",
    "chunks={\"time\": 6, \"lon\": 703, \"lat\": 311}\n",
    "pr = [xr.open_dataset(fs.open(f\"s3://nhgf-development/thredds/prism_v2/prism_{str(year)}.nc\"), chunks=chunks, decode_coords=\"all\") for year in prism_years]\n",
    "prism = xr.concat(pr, dim=\"time\")\n",
    "prism = prism.drop_vars(\"time_bnds\")\n",
    "# prism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing using AlbersEqualArea to write crs for prism so it displays properly after transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prism_crs = ccrs.AlbersEqualArea()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prism.rio.write_crs(4269, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename the dimensions to match cd conventions used by rioxarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prism = prism.rename({\"lon\":\"x\", \"lat\":\"y\"})\n",
    "# prism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prism.hvplot(x=\"x\", y=\"y\", geo=True, rasterize=True, tiles=\"OSM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reproject from NAD83 to Lambert Conformal Conic (matches CONUS404 dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prism_reproject = prism.rio.reproject(crs, num_threads=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prism_reproject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "xmin, ymin = prism_reproject.x[0].values, prism_reproject.y[0].values\n",
    "\n",
    "y = ymin - np.arange(872)*4000.\n",
    "\n",
    "x = xmin + np.arange(1602)*4000.\n",
    "\n",
    "prism_reproject = prism_reproject.assign_coords(x=x, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prism_reproject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prism_reproject.hvplot(x=\"x\", y=\"y\", rasterize=True,,tiles=\"OSM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clip to DRB\n",
    "prism_drb = prism_reproject.rio.clip(drb.geometry, crs=crs, drop=True, invert=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the clipped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prism_drb.hvplot.quadmesh(x='x', y='y', rasterize=True, tiles='OSM', alpha=0.7, cmap='turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del prism_reproject\n",
    "# del prism_drb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Last code cell of the notebook\n",
    "# import watermark.watermark as watermark\n",
    "# print(watermark(iversions=True, python=True, machine=True, globals_=globals()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close(); cluster.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "users-pangeo",
   "language": "python",
   "name": "conda-env-users-pangeo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "6b8b815d206080047d0881750c260f2e84eb4576ca4137e2355fa3de469693c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
