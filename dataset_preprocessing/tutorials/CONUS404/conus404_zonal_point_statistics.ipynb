{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create zonal statistics and point extractions for comparing CONUS404 and reference datasets\n",
    "\n",
    "Short paragraph describing what is about to happen\n",
    "\n",
    "<details>\n",
    "  <summary>Guide to pre-requisites and learning outcomes...&lt;click to expand&gt;</summary>\n",
    "  \n",
    "  <table>\n",
    "    <tr>\n",
    "      <td>Pre-Requisites\n",
    "      <td>To get the most out of this notebook, you should already have an understanding of these topics: \n",
    "        <ul>\n",
    "        <li>pre-req one\n",
    "        <li>pre-req two\n",
    "        </ul>\n",
    "    <tr>\n",
    "      <td>Expected Results\n",
    "      <td>At the end of this notebook, you should be able to: \n",
    "        <ul>\n",
    "        <li>outcome one\n",
    "        <li>outcome two\n",
    "        </ul>\n",
    "  </table>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library imports\n",
    "import fsspec #testing\n",
    "import hvplot.xarray #testing\n",
    "import intake #testing\n",
    "import os #testing\n",
    "import warnings #testing\n",
    "import rioxarray #testing\n",
    "import dask #testing\n",
    "import metpy #testing\n",
    "import calendar #testing\n",
    "\n",
    "from shapely.geometry import Polygon #testing\n",
    "from dask.distributed import LocalCluster, Client #testing\n",
    "from pygeohydro import pygeohydro #testing\n",
    "from fsspec.implementations.ftp import FTPFileSystem #testing\n",
    "from holoviews.streams import PolyEdit, PolyDraw #testing\n",
    "from geocube.api.core import make_geocube #testing\n",
    "\n",
    "import xarray as xr #testing\n",
    "import geopandas as gpd #testing\n",
    "import pandas as pd #testing\n",
    "import geoviews as gv #testing\n",
    "import dask.dataframe as dd #testing\n",
    "import numpy as np #testing\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update to helper function after repo consolidation\n",
    "## **Start a Dask client using an appropriate Dask Cluster** \n",
    "This is an optional step, but can speed up data loading significantly, especially when accessing data from the cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_cluster(machine):\n",
    "    ''' Helper function to configure cluster\n",
    "    '''\n",
    "    if machine == 'denali':\n",
    "        from dask.distributed import LocalCluster, Client\n",
    "        cluster = LocalCluster(threads_per_worker=1)\n",
    "        client = Client(cluster)\n",
    "    \n",
    "    elif machine == 'tallgrass':\n",
    "        from dask.distributed import Client\n",
    "        from dask_jobqueue import SLURMCluster\n",
    "        cluster = SLURMCluster(queue='cpu', cores=1, interface='ib0',\n",
    "                               job_extra=['--nodes=1', '--ntasks-per-node=1', '--cpus-per-task=1'],\n",
    "                               memory='6GB')\n",
    "        cluster.adapt(maximum_jobs=30)\n",
    "        client = Client(cluster)\n",
    "        \n",
    "    elif machine == 'local':\n",
    "        import os\n",
    "        import warnings\n",
    "        from dask.distributed import LocalCluster, Client\n",
    "        warnings.warn(\"Running locally can result in costly data transfers!\\n\")\n",
    "        n_cores = os.cpu_count() # set to match your machine\n",
    "        cluster = LocalCluster(threads_per_worker=n_cores)\n",
    "        client = Client(cluster)\n",
    "        \n",
    "    elif machine in ['esip-qhub-gateway-v0.4']:   \n",
    "        import sys, os\n",
    "        sys.path.append(os.path.join(os.environ['HOME'],'shared','users','lib'))\n",
    "        import ebdpy as ebd\n",
    "        aws_profile = 'nhgf-development'\n",
    "        ebd.set_credentials(profile=aws_profile)\n",
    "\n",
    "        aws_region = 'us-west-2'\n",
    "        endpoint = f's3.{aws_region}.amazonaws.com'\n",
    "        ebd.set_credentials(profile=aws_profile, region=aws_region, endpoint=endpoint)\n",
    "        worker_max = 30\n",
    "        client,cluster = ebd.start_dask_cluster(profile=aws_profile, worker_max=worker_max, \n",
    "                                              region=aws_region, use_existing_cluster=True,\n",
    "                                              adaptive_scaling=False, wait_for_cluster=False, \n",
    "                                              worker_profile='Medium Worker', propagate_env=True)\n",
    "        \n",
    "    return client, cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup your cluster\n",
    "\n",
    "#### QHub...\n",
    "Uncomment single commented spaces (#) to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set machine\n",
    "machine = 'esip-qhub-gateway-v0.4'\n",
    "\n",
    "# use configure cluster helper function to setup dask\n",
    "client, cluster = configure_cluster(machine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### or HPC\n",
    "Uncomment single commented spaces (#) to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set machine\n",
    "# machine = os.environ['SLURM_CLUSTER_NAME']\n",
    "\n",
    "## use configure_cluster helper function to setup dask\n",
    "# client, cluster = configure_cluster(machine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Compute zonal statistics for gridded datasets**\n",
    "\n",
    "In the last tutorial, we prepared three gridded datasets: CONUS404 (benchmark), PRISM (reference), and CERES-EBAF (reference). The goal of this section is compute [zonal statistics](https://gisgeography.com/zonal-statistics/) for each HUC6 zone in the Delaware River Basin (DRB) at each time-step in the data. This tabular data will then be exported for use in the next notebook, **CONUS404 Analysis**.\n",
    "\n",
    "The steps for each dataset will be something like this:\n",
    "1. Read in the prepared dataset\n",
    "2. Read in the HUC6 boundaries and transform to same coordinate reference system as prepared dataset\n",
    "3. Make a data mask with the HUC6 boundaries to calculate zonal statistics\n",
    "4. Compute zonal statistics with data mask and prepared data\n",
    "\n",
    "Once the first 4 steps have completed for each dataset: \n",
    "\n",
    "5. Combine all three zonal statistics into single dataset\n",
    "6. Export gridded data zonal statistics\n",
    "<br>\n",
    "\n",
    "**CONUS404 zonal statistics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# url to c404_drb\n",
    "c404_drb_url = 's3://nhgf-development/workspace/tutorial/CONUS404/c404_drb.nc'\n",
    "\n",
    "# open dataset\n",
    "c404_drb = xr.open_dataset(c404_drb_url, cache=False, decode_cf=True, decode_coords=\"all\")\n",
    "\n",
    "c404_drb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the CRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set crs\n",
    "c404_crs = c404_drb[\"RNET\"].metpy.cartopy_crs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in HUC6 boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geometry</th>\n",
       "      <th>huc6</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POLYGON ((-74.59840 42.45930, -74.60130 42.461...</td>\n",
       "      <td>020401</td>\n",
       "      <td>Upper Delaware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POLYGON ((-75.93127 40.88437, -75.93681 40.885...</td>\n",
       "      <td>020402</td>\n",
       "      <td>Lower Delaware</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            geometry    huc6            name\n",
       "0  POLYGON ((-74.59840 42.45930, -74.60130 42.461...  020401  Upper Delaware\n",
       "1  POLYGON ((-75.93127 40.88437, -75.93681 40.885...  020402  Lower Delaware"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# bring in HUC6 boundaries found in the DRB\n",
    "drb = pygeohydro.WBD(\"huc6\", outfields=[\"huc6\", \"name\"]).byids(\"huc6\", [\"020401\", \"020402\"])\n",
    "\n",
    "# set CRS to match c404_drb\n",
    "# drb = drb.to_crs(c404_crs)\n",
    "\n",
    "#visualize\n",
    "drb.plot(edgecolor=\"orange\", facecolor=\"purple\", linewidth=2.5)\n",
    "\n",
    "drb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create datamask and build new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert huc6 field to int as this works best for the following steps\n",
    "drb[\"huc6\"] = drb[\"huc6\"].astype(int) #note: this may drop the # of digits from 6 to less depending on how many zeroes there were, may need to pad back to 6 digits later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "geometry    geometry\n",
       "huc6           int32\n",
       "name          object\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create an output grid\n",
    "c404_out_grid = make_geocube(\n",
    "    vector_data = drb,\n",
    "    measurements=[\"huc6\"],\n",
    "    like=c404_drb\n",
    ")\n",
    "\n",
    "# add datarrays to grid\n",
    "c404_out_grid[\"RNET\"] = (c404_drb.RNET.dims, c404_drb.RNET.values, \n",
    "                         c404_drb.RNET.attrs, c404_drb.RNET.encoding)\n",
    "\n",
    "c404_out_grid[\"TK\"] = (c404_drb.TK.dims, c404_drb.TK.values,\n",
    "                         c404_drb.TK.attrs, c404_drb.TK.encoding)\n",
    "\n",
    "c404_out_grid[\"PREC_NC_ACC\"] = (c404_drb.PREC_NC_ACC.dims, c404_drb.PREC_NC_ACC.values,\n",
    "                         c404_drb.PREC_NC_ACC.attrs, c404_drb.PREC_NC_ACC.encoding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group data arrays by HUC6 code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_grouped = c404_out_grid.drop(\"spatial_ref\").groupby(c404_out_grid.huc6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Putting it together: Process CONUS404 to variable and research spatial extent**\n",
    "In this section we are going to put together some skills we have learned so far: bring in CONUS404, select our variables, then clip to our spatial extent. This assumes that the notebook is being run on the ESIP QHub. If being run on HPC then comment/uncomment the datasets as needed.\n",
    "\n",
    "Variables: Accumulated precipitation (PREC_ACC_NC), air temperature (TK), and (calculated) surface net radiation (RNET) <br>\n",
    "Spatial extent: Delaware River Basin (DRB)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set up conus404 filename\n",
    "conus404 = 'conus404-hourly-cloud' #ESIP QHub\n",
    "# conus404 = 'conus404-hourly-onprem' #HPC\n",
    "\n",
    "# create dask array from dataset\n",
    "ds = cat[conus404].to_dask()\n",
    "\n",
    "# parse spatial information from CF conventions\n",
    "ds = ds.metpy.parse_cf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the coordinate reference system (CRS) information from CONUS404 dataset to use when setting the CRSs for other datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crs = ds[\"TK\"].metpy.cartopy_crs\n",
    "# crs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other datasets that that are brought in might need to be sliced to the same time period as the CONUS404 dataset. Set the start and end dates for the CONUS404 dataset to use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the minimum time coordinate\n",
    "start_date = ds.coords[\"time\"].values.min()\n",
    "\n",
    "# convert to datetime then extract the year-month as a string \"YYYYmm\"\n",
    "start_date = pd.to_datetime(start_date).strftime(\"%Y-%m\")\n",
    "\n",
    "# add the first day of the month to the date\n",
    "start_date = f\"{start_date}-01\"\n",
    "\n",
    "# get the maximum time coordinate\n",
    "end_date = ds.coords[\"time\"].values.max()\n",
    "\n",
    "# convert to datetime then extract the year-month as a string \"YYYYmm\"\n",
    "end_date = pd.to_datetime(end_date).strftime(\"%Y-%m\")\n",
    "\n",
    "# as the end date of months vary, use the monthrange function, which returns a tuple of integers as (firstDay, lastDay)\n",
    "# extract the lastDay integer by indexing [1] and convert it to string\n",
    "last_day = calendar.monthrange(int(end_date[0:4]), int(end_date[-2:]))[1]\n",
    "last_day = str(last_day) \n",
    "\n",
    "# add last_data to end_date                         \n",
    "end_date = f\"{end_date}-{last_day}\"\n",
    "\n",
    "print(\"Start date:\", start_date, \"\\nEnd date:\", end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now bring in the AOI boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a subset of desired data variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#subset data variables\n",
    "c404_variables = [\"PREC_ACC_NC\", \"TK\", \"ACSWDNB\", \"I_ACSWDNB\", \"ACSWUPB\", \"I_ACSWUPB\", \"ACLWDNB\", \"I_ACLWDNB\", \"ACLWUPB\", \"I_ACLWUPB\"]\n",
    "c404 = ds[c404_variables]\n",
    "\n",
    "# write CRS\n",
    "c404.rio.write_crs(crs, inplace=True)\n",
    "\n",
    "# perform clip\n",
    "c404_drb = c404.rio.clip(drb.geometry, crs=crs, drop=True, invert=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# c404_drb[\"ACSWDNB\"].isel(time=-1).hvplot(x='x', y='y', crs=crs, rasterize=True, cmap='turbo', tiles='OSM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a little more processing to do before the dataset is ready for analysis. We need to:\n",
    "1. Calcuate RNET using the radiation columns\n",
    "2. Resample and aggregate the data to the desired time-step (1 month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNET is calculated using the equation <br>\n",
    "RNET = SWDN + LWDN - SWUP - LWUP\n",
    "\n",
    "SWDN = downwelling surface flux of shortwave radiation <br>\n",
    "LWDN = downwelling surface flux of longwave radiation <br>\n",
    "SWUP = upwelling surface flux of shortwave radiation <br>\n",
    "LWUP = upwelling surface flux of longwave radiation <br>\n",
    "\n",
    "Each of the RNET component pieces is calculated using different data variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets break down the components. First, let's tackle SWDN, which is computed from ACSWDNB and I_ACSWDNB (accumulations in units of J/m2).  To compute SWDN in W/m2\n",
    "over one hour we use:\n",
    "\n",
    "(ACSWDNB[h]+(1e9xI_ACSWDNB[h])) - (ACSWDNB[h-1]+(1e9xI_ACSWDNB[h-1])) / 3600\n",
    "\n",
    "which uses the data variables at the time hour (h) and the previous hour (h-1). The I_ variables are also scaled by multiplying by 1e9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want all values for ACSWDNB and I_ACSWDNB starting at time index h where h is the second time step since indices start at 0...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ACSWDNB = c404_drb[\"ACSWDNB\"][1:]\n",
    "\n",
    "I_ACSWDNB = c404_drb[\"I_ACSWDNB\"][1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we want all values for ACSWDNB and I_ACSWDNB at time index h-1. However, we want the time indexes for both our h and h-1 datasets to be the same (this makes a difference in calculations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACSWDNB1 = c404_drb[\"ACSWDNB\"][:-1]\n",
    "ACSWDNB1.coords[\"time\"] = ACSWDNB.coords[\"time\"]\n",
    "\n",
    "I_ACSWDNB1 = c404_drb[\"I_ACSWDNB\"][:-1]\n",
    "I_ACSWDNB1.coords[\"time\"] = I_ACSWDNB.coords[\"time\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm both time coords are the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ACSWDNB.coords[\"time\"].values) == len(ACSWDNB1.coords[\"time\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiply the *I_* arrays by 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# h\n",
    "I_ACSWDNB_1e9 = 1e9*I_ACSWDNB\n",
    "\n",
    "# h-1\n",
    "I_ACSWDNB1_1e9 = 1e9*I_ACSWDNB1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculate SWDN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SWDN = ((ACSWDNB + I_ACSWDNB_1e9) - (ACSWDNB1 + I_ACSWDNB1_1e9)) / 3600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize some of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SWDN.sel(time=\"2000-06-01 10:00\").hvplot(x='x', y='y', crs=crs, rasterize=True, cmap='turbo', tiles='OSM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SWDN.sel(time=\"2000-06-01 23:00\").hvplot(x='x', y='y', crs=crs, rasterize=True, cmap='turbo', tiles='OSM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, pad a NaN to the beginning to match original datasets dimension length and then reset to those dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SWDN = SWDN.pad({\"time\":(1,0)})\n",
    "SWDN.coords[\"time\"] = c404_drb[\"ACSWDNB\"].coords[\"time\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will do the same steps to calculate the other three building blocks of *RNET*.\n",
    "\n",
    "*SWUP*\n",
    "\n",
    "(ACSWUPB[h]+(1e9xI_ACSWUPB[h])) - (ACSWUPB[h-1]+(1e9xI_ACSWUPB[h-1])) / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (h) variables\n",
    "ACSWUPB = c404_drb[\"ACSWUPB\"][1:]\n",
    "\n",
    "I_ACSWUPB = c404_drb[\"I_ACSWUPB\"][1:]\n",
    "\n",
    "# (h-1) variables)\n",
    "ACSWUPB1 = c404_drb[\"ACSWUPB\"][:-1]\n",
    "ACSWUPB1.coords[\"time\"] = ACSWUPB.coords[\"time\"]\n",
    "\n",
    "I_ACSWUPB1 = c404_drb[\"I_ACSWUPB\"][:-1]\n",
    "I_ACSWUPB1.coords[\"time\"] = I_ACSWUPB.coords[\"time\"]\n",
    "\n",
    "# modify bucket variables by 1e9\n",
    "I_ACSWUPB_1e9 = 1e9*I_ACSWUPB\n",
    "I_ACSWUPB1_1e9 = 1e9*I_ACSWUPB1\n",
    "\n",
    "# calculate variable\n",
    "SWUP = ((ACSWUPB + I_ACSWUPB_1e9) - (ACSWUPB1 + I_ACSWUPB1_1e9)) / 3600\n",
    "\n",
    "# pad to match c404_drb time dimension\n",
    "SWUP = SWUP.pad({\"time\":(1,0)})\n",
    "SWUP.coords[\"time\"] = c404_drb[\"ACSWUPB\"].coords[\"time\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*LWDN*\n",
    "\n",
    "(ACLWDNB[h]+(1e9xI_ACLWDNB[h])) - (ACLWDNB[h-1]+(1e9xI_ACLWDNB[h-1])) / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (h) variables\n",
    "ACLWDNB = c404_drb[\"ACLWDNB\"][1:]\n",
    "\n",
    "I_ACLWDNB = c404_drb[\"I_ACLWDNB\"][1:]\n",
    "\n",
    "# (h-1) variables)\n",
    "ACLWDNB1 = c404_drb[\"ACLWDNB\"][:-1]\n",
    "ACLWDNB1.coords[\"time\"] = ACLWDNB.coords[\"time\"]\n",
    "\n",
    "I_ACLWDNB1 = c404_drb[\"I_ACLWDNB\"][:-1]\n",
    "I_ACLWDNB1.coords[\"time\"] = I_ACLWDNB.coords[\"time\"]\n",
    "\n",
    "# modify bucket variables by 1e9\n",
    "I_ACLWDNB_1e9 = 1e9*I_ACLWDNB\n",
    "I_ACLWDNB1_1e9 = 1e9*I_ACLWDNB1\n",
    "\n",
    "# calculate variable\n",
    "LWDN = ((ACLWDNB + I_ACLWDNB_1e9) - (ACLWDNB1 + I_ACLWDNB1_1e9)) / 3600\n",
    "\n",
    "# pad to match c404_drb time dimension\n",
    "LWDN = LWDN.pad({\"time\":(1,0)})\n",
    "LWDN.coords[\"time\"] = c404_drb[\"ACLWDNB\"].coords[\"time\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*LWUP*\n",
    "\n",
    "(ACLWUPB[h]+(1e9xI_ACLWUPB[h])) - (ACLWUPB[h-1]+(1e9xI_ACLWUPB[h-1])) / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (h) variables\n",
    "ACLWUPB = c404_drb[\"ACLWUPB\"][1:]\n",
    "\n",
    "I_ACLWUPB = c404_drb[\"I_ACLWUPB\"][1:]\n",
    "\n",
    "# (h-1) variables)\n",
    "ACLWUPB1 = c404_drb[\"ACLWUPB\"][:-1]\n",
    "ACLWUPB1.coords[\"time\"] = ACLWUPB.coords[\"time\"]\n",
    "\n",
    "I_ACLWUPB1 = c404_drb[\"I_ACLWUPB\"][:-1]\n",
    "I_ACLWUPB1.coords[\"time\"] = I_ACLWUPB.coords[\"time\"]\n",
    "\n",
    "# modify bucket variables by 1e9\n",
    "I_ACLWUPB_1e9 = 1e9*I_ACLWUPB\n",
    "I_ACLWUPB1_1e9 = 1e9*I_ACLWUPB1\n",
    "\n",
    "# calculate variable\n",
    "LWUP = ((ACLWUPB + I_ACLWUPB_1e9) - (ACLWUPB1 + I_ACLWUPB1_1e9)) / 3600\n",
    "\n",
    "# pad to match c404_drb time dimension\n",
    "LWUP = LWUP.pad({\"time\":(1,0)})\n",
    "LWUP.coords[\"time\"] = c404_drb[\"ACLWUPB\"].coords[\"time\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the parts, calculate RNET.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculate\n",
    "RNET = SWDN + LWDN - SWUP - LWUP\n",
    "# RNET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...assign its attributes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of attributes\n",
    "RNET_attrs = {'description': 'NET RADIATION FROM PAST HOUR FOR BUCKET',\n",
    " 'grid_mapping': 'crs',\n",
    " 'long_name': 'Bucket net radiation',\n",
    " 'units': 'W m-2'\n",
    "}\n",
    "\n",
    "# assign attributes\n",
    "RNET = RNET.assign_attrs(RNET_attrs)\n",
    "# RNET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " and assign it back to CONUS404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_drb = c404_drb.assign(RNET=RNET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now drop the extra radiation variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_variables_drop = [\"ACSWDNB\", \"I_ACSWDNB\", \"ACSWUPB\", \"I_ACSWUPB\", \"ACLWDNB\", \"I_ACLWDNB\", \"ACLWUPB\", \"I_ACLWUPB\"]\n",
    "c404_drb = c404_drb.drop_vars(c404_variables_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize RNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c404_drb[\"RNET\"].sel(time=\"2000-06-01 23:00\").hvplot(x='x', y='y', crs=crs, rasterize=True, cmap='turbo', tiles='OSM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset has been clipped to the area of interest and all the needed variables calculated. The final bit of engineering is resampling the data from hourly to monthly. Xarray has a built in method `resample()` to do this but it only allows a single aggregation method for all the DataArrays in the DataSet. \n",
    "\n",
    "Unfortunately, the DataArrays need different aggregation techniques: sum for *PREC_ACC_NC* and mean for *RNET* and *TK*. We'll accomplish this by splitting *PREC_ACC_NC* from the dataset, resampling it and the dataset seperately, then merging them back together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy data\n",
    "PREC_ACC_NC = c404_drb[\"PREC_ACC_NC\"]\n",
    "\n",
    "# resample to 1 month by summing\n",
    "PREC_ACC_NC = PREC_ACC_NC.resample(time=\"1M\").sum()\n",
    "\n",
    "# copy attributes from original\n",
    "PREC_ACC_NC.attrs = c404_drb[\"PREC_ACC_NC\"].attrs\n",
    "\n",
    "# drop from c404_drb\n",
    "c404_drb = c404_drb.drop_vars(\"PREC_ACC_NC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resample the dataset and aggregate by mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with xr.set_options(keep_attrs=True): # needed, otherwise drops attributes\n",
    "    c404_drb = c404_drb.resample(time=\"1M\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add back the resampled *PREC_ACC_NC*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_drb[\"PREC_ACC_NC\"] = PREC_ACC_NC\n",
    "# c404_drb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct attributes as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_drb.PREC_ACC_NC.attrs[\"integration_length\"] = 'accumulated over prior month'\n",
    "c404_drb.RNET.attrs[\"description\"] = \"MEAN RADIATION FROM PAST MONTH FOR BUCKET\"\n",
    "c404_drb.TK.attrs[\"description\"] = \"MEAN AIR TEMPERATURE AT THE LOWEST MODEL LEVEL OVER THE PREVIOUS MONTH\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop coordinates that will not be need in the exported dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_drb = c404_drb.reset_coords([\"metpy_crs\", \"crs\"], drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set chunk size so entire dataset is single chunck (the data is small enought)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_drb = c404_drb.chunk(chunks={\n",
    "    \"y\":103,\n",
    "    \"x\":46,\n",
    "    \"time\":492\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the final prepared dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_drb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export data as a NetCDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = fsspec.open('simplecache::s3://nhgf-development/workspace/tutorial/CONUS404/c404_drb.nc', \n",
    "                      mode='wb', s3=dict(profile='nhgf-development'))\n",
    "with outfile as f:\n",
    "    c404_drb.load().to_netcdf(f, compute=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Prepare reference data\n",
    "\n",
    "Now that the CONUS404 dataset has been preprocessed, we need to do the same with datasets used for comparison with the forcings data. In this section, data will be brought in from several sources and preprocessed in data-type-appropriate ways.\n",
    "\n",
    "We'll start by processing station data.\n",
    "\n",
    "#### NOAA's Global Historical Climate Network - Daily (GHCN) Dataset\n",
    "It is always important to review any readme or metadata files for the data you wish to bring in. The [GHCN readme](https://noaa-ghcn-pds.s3.amazonaws.com/readme.txt) is useful because it explains what is in the S3 bucket, the various columns in the datasets, and other information. When we later call in the observational data, the [by station readme](https://noaa-ghcn-pds.s3.amazonaws.com/readme-by_station.txt) provides a more detailed explanation of the data there.\n",
    "\n",
    "The steps for working the the GHCN data will be that we first read in the data that describe the stations (metadata); then we'll read in time-series data for each station.\n",
    "\n",
    "After reading the metadata for the file, we can see that only the first three columns are needed to map the stations: the station ID, latitude, and longitude. However, we want to make sure that we are only using GHCN stations so we need to also use the (G)HCN/CRN Flag column to filter to GHCN sites. \n",
    "\n",
    "Start by getting a list of stations from the AWS S3 bucket where the daily data is housed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghcn_all = pd.read_csv('s3://noaa-ghcn-pds/ghcnd-stations.txt', sep=\"\\t\", header=None)\n",
    "# ghcn_all.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the read does not recognize the file's columns and the dare are stored as one column per record. So, we have to split the record to create the columns that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghcn_all = ghcn_all[0].str.split(\" +\",expand = True)\n",
    "# ghcn_all.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns 0-3 now look as we'd expect. However, column 4 is where it starts to get messy as the method for expanding the columns has split up the station names at the spaces between. This means that the HCN flag, which we would expect to be in column 6, could be in columns 6-13. Thankfully, the pandas ```loc``` function makes this filtering easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ushcn = ghcn_all.loc[(ghcn_all[6] == \"HCN\") | (ghcn_all[7] == \"HCN\") | (ghcn_all[8] == \"HCN\") | (ghcn_all[9] == \"HCN\") | (ghcn_all[10] == \"HCN\") | (ghcn_all[11] == \"HCN\") | (ghcn_all[12] == \"HCN\") | (ghcn_all[13] == \"HCN\")].copy()\n",
    "ushcn = ushcn.iloc[:, 0:3].rename({0:\"station\", 1:\"lat\", 2:\"lon\"}, axis=1).copy() # after the search, trim the columns and rename to get the data to what is needed to map\n",
    "# ushcn.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to clip the points to only those in the DRB. We do that by using the latitude and longitude to create a GeoDataFrame..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ushcn_gdf = gpd.GeoDataFrame(ushcn, geometry=gpd.points_from_xy(ushcn['lon'], ushcn['lat'], crs=\"EPSG:4326\"))\n",
    "\n",
    "# convert to same crs as drb\n",
    "ushcn_gdf = ushcn_gdf.to_crs(crs)\n",
    "\n",
    "# ushcn_gdf.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...followed by clipping using the *drb* geodataframe above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hcn_drb_gdf = gpd.clip(ushcn_gdf, drb)\n",
    "# hcn_drb_gdf.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to pull in the tabular data for all of the DRB stations. These are stored on AWS in an individual CSV for each station named *station.csv*. So, we need to get all of the station IDs from our dataset and use them to create a list of URLs for these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hcn_drb_data_url = [f\"s3://noaa-ghcn-pds/csv/by_station/{station}.csv\" for station in hcn_drb_gdf[\"station\"].unique().tolist()]\n",
    "# print(hcn_drb_data_url[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(hcn_drb_data_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now pass that list of URLs to *dask.dataframe.read_csv*, which will read the data in parallel. We'll then refine the entries to in the temporal range of CONUS404."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "storage_options = dict(anon=True, requester_pays=False)\n",
    "hcn_drb_data = dd.read_csv(hcn_drb_data_url, parse_dates=[\"DATE\"], \n",
    "                           usecols=[\"ID\", \"DATE\", \"ELEMENT\", \"DATA_VALUE\"], storage_options=storage_options)\n",
    "\n",
    "hcn_drb_data = hcn_drb_data.loc[(hcn_drb_data[\"DATE\"] >= start_date) & (hcn_drb_data[\"DATE\"] <= end_date)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Next, we'll refine the dataframe by a list of elements and then compute it.\n",
    "\n",
    "##### Note: We are using TMAX and TMIN rather than TAVG as TAVG has no records prior to 1998."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of elements we are interested in\n",
    "element_list = [\"PRCP\", \"TMAX\", \"TMIN\"]\n",
    "\n",
    "hcn_drb_data = hcn_drb_data.loc[hcn_drb_data[\"ELEMENT\"].isin(element_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check shape\n",
    "# hcn_drb_data.compute().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how much memory does it take up?\n",
    "# hcn_drb_data.compute().memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dask dataframe is about 21 mb in size.\n",
    "\n",
    "Similar to the CONUS404 data, we have a little more engineering to do with the data. We need to calculate the average temperatue using TMIN and TMAX (in Kelvin) as well as resample the data to a 1 month interval. We'll convert the Dask Dataframe into a Pandas Dataframe to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcn_drb_data_df = hcn_drb_data.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by whittling down to our two temperature elements, dropping the *ELEMENT* column, and grouping our data by *ID* and *DATE* in order to take the mean of *TMIN* and *TMAX* and convert this to degrees Kelvin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# paring down data\n",
    "hcn_drb_tk = hcn_drb_data_df.loc[hcn_drb_data_df[\"ELEMENT\"].isin([\"TMAX\",\"TMIN\"])]\n",
    "\n",
    "# dropping ELEMENT\n",
    "hcn_drb_tk = hcn_drb_tk.drop(\"ELEMENT\", axis=1)\n",
    "\n",
    "# calculate mean temperature for each station and date\n",
    "hcn_drb_tk = hcn_drb_tk.groupby([\"ID\", \"DATE\"]).mean()\n",
    "\n",
    "# rename the DATA_VALUE column to TK\n",
    "hcn_drb_tk = hcn_drb_tk.rename({\"DATA_VALUE\":\"TK\"}, axis=1)\n",
    "\n",
    "# convert from tenths of degrees Celsius to degrees Kelvin\n",
    "hcn_drb_tk[\"TK\"] = (hcn_drb_tk[\"TK\"] * 0.1) + 273.15\n",
    "\n",
    "# reset the index\n",
    "hcn_drb_tk.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isolate the *PRCP* element and rename like TK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcn_drb_prcp = hcn_drb_data_df.loc[hcn_drb_data_df[\"ELEMENT\"] == \"PRCP\"].copy()\n",
    "\n",
    "# dropping ELEMENT\n",
    "hcn_drb_prcp = hcn_drb_prcp.drop(\"ELEMENT\", axis=1)\n",
    "\n",
    "# rename the DATA_VALUE column to PREC_ACC_NC\n",
    "hcn_drb_prcp = hcn_drb_prcp.rename({\"DATA_VALUE\":\"PREC_ACC_NC\"}, axis=1)\n",
    "\n",
    "# convert from tenths of mm to mm\n",
    "hcn_drb_prcp[\"PREC_ACC_NC\"] = hcn_drb_prcp[\"PREC_ACC_NC\"] * 0.1\n",
    "\n",
    "# reset the index\n",
    "hcn_drb_prcp.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# hcn_drb_prcp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine *TK* and *PRCP* DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcn_drb = hcn_drb_tk.merge(hcn_drb_prcp, how=\"inner\", on=[\"ID\", \"DATE\"])\n",
    "# hcn_drb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then resample to 1 month and aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcn_drb = hcn_drb.groupby(\"ID\").resample(\"1M\", on=\"DATE\").agg({\"TK\":\"mean\", \"PREC_ACC_NC\":\"sum\"}).reset_index(drop=False)\n",
    "\n",
    "# round TK\n",
    "hcn_drb.TK = round(hcn_drb.TK, 2)\n",
    "# hcn_drb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now add the latitude and longitude coordinates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcn_drb_coords = pd.DataFrame(hcn_drb_gdf.drop(columns=\"geometry\"))\n",
    "\n",
    "# rename ID columnt o match drb_hcn\n",
    "hcn_drb_coords = hcn_drb_coords.rename({\"station\": \"ID\", \"lon\":\"LONGITUDE\", \"lat\":\"LATITUDE\"}, axis=1)\n",
    "\n",
    "# merge\n",
    "hcn_drb = hcn_drb.merge(hcn_drb_coords, on=\"ID\", how=\"left\")\n",
    "\n",
    "hcn_drb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "Export the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcn_drb.to_parquet(\"s3://nhgf-development/workspace/tutorial/CONUS404/hcn_drb.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOAA's Global Climate Reference Network (GCRN) Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use *fsspec* to make FTP call to NOAA for CRN data <br>\n",
    "First, create file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = FTPFileSystem(\"ftp.ncei.noaa.gov\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the file type is *tab-separated values (tsv)*, we will use the *pd.read_table* function to create a Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uscrn_all = pd.read_table(fs.open(\"/pub/data/uscrn/products/stations.tsv\")) \n",
    "uscrn_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now turn into GeoDataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uscrn_gdf = gpd.GeoDataFrame(uscrn_all, geometry=gpd.points_from_xy(uscrn_all[\"LONGITUDE\"], uscrn_all[\"LATITUDE\"]), crs=\"EPSG:4326\")\n",
    "\n",
    "# convert to same crs as drb\n",
    "uscrn_gdf = uscrn_gdf.to_crs(crs)\n",
    "\n",
    "# uscrn_gdf.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find which USCRN sites are in DRB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "crn_drb_gdf = gpd.clip(uscrn_gdf, drb)\n",
    "# crn_drb_gdf.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crn_drb_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now know what CRN sites are in the Delaware River Basin. We must now retrieve the data for this site from the FTP server.\n",
    "\n",
    "First, we'll get the location name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crn_stat_name = crn_drb_gdf[\"LOCATION\"].values.tolist()[0]\n",
    "print(crn_stat_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initilize the FTP connection again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = FTPFileSystem(\"ftp.ncei.noaa.gov\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list_glob = fs.glob(\n",
    "    f\"/pub/data/uscrn/products/daily01/**/*{crn_stat_name}*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crn_drb = pd.DataFrame()\n",
    "\n",
    "for file in file_list_glob:\n",
    "    stat_data = pd.read_csv(fs.open(file), header=None, sep=\"\\t\")\n",
    "    crn_drb = pd.concat([crn_drb, stat_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crn_drb = crn_drb[0].str.split(\" +\",expand = True)\n",
    "# crn_drb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now bring in the headers for the station data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crn_headers = fs.open(\"/pub/data/uscrn/products/daily01/headers.txt\")\n",
    "crn_data_headers = pd.read_csv(crn_headers, sep=\"\\t\", header=None).iloc[1,:].str.split(\" +\").values.tolist()[0][0:28]\n",
    "# crn_data_headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the number of headers equals the number of columns in our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(crn_drb.columns) == len(crn_data_headers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now rename the column headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crn_drb.columns = crn_data_headers\n",
    "# crn_drb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *LST_DATE* column is the date of the observation and its data type is currently string. We want the data type to be [datetime](https://docs.python.org/3/library/datetime.html) so we will perform this conversion using the `to_datetime` function from the `pandas` library then refine the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crn_drb[\"DATE\"] = pd.to_datetime(crn_drb[\"LST_DATE\"])\n",
    "crn_drb = crn_drb[[\"DATE\", \"P_DAILY_CALC\", \"T_DAILY_AVG\", \"SOLARAD_DAILY\", \"LONGITUDE\", \"LATITUDE\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you examine the data types, you'll see that the 4 columns of numbers are actually data type *object* when we need them as numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crn_drb.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets rectify that by applying the `pd.to_numeric` function to the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = crn_drb.columns.drop(\"DATE\")\n",
    "crn_drb[cols] = crn_drb[cols].apply(pd.to_numeric, errors='coerce')\n",
    "crn_drb.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now refine the data by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crn_drb = crn_drb.loc[(crn_drb[\"DATE\"] >= start_date) & (crn_drb[\"DATE\"] <= end_date)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CRN dataset has many values of -9999.0, which is where a record was not recorded due to data quality or other issues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crn_drb.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the value -9999.0 is seen as a valid number, set these values to NaN so they will be ignored during calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to NaN\n",
    "crn_drb = crn_drb.replace(-9999.0, np.nan)\n",
    "\n",
    "# add a station ID column\n",
    "crn_drb[\"ID\"] = crn_stat_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now convert columns to the correct units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Celsius to Kelvin\n",
    "crn_drb[\"TK\"] = crn_drb[\"T_DAILY_AVG\"] + 273.15\n",
    "\n",
    "# MJ/m2 to W/m2\n",
    "crn_drb[\"RNET\"] = (crn_drb[\"SOLARAD_DAILY\"]*1e6) / 86400 # converts MJ/m2 to W/m2\n",
    "\n",
    "# drop columns\n",
    "crn_drb = crn_drb.drop([\"T_DAILY_AVG\",\"SOLARAD_DAILY\"], axis=1)\n",
    "\n",
    "# rename column\n",
    "crn_drb = crn_drb.rename({\"P_DAILY_CALC\": \"PREC_ACC_NC\"}, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resample the data and round TK and RNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crn_drb = crn_drb.groupby(\"ID\").resample(\"1M\", on=\"DATE\").agg({\"TK\":\"mean\", \"PREC_ACC_NC\":\"sum\", \"RNET\":\"mean\", \"LATITUDE\":\"mean\", \"LONGITUDE\":\"mean\"}).reset_index(drop=False)\n",
    "\n",
    "# round \n",
    "crn_drb[\"RNET\"] = round(crn_drb[\"RNET\"], 2)\n",
    "crn_drb[\"TK\"] = round(crn_drb[\"TK\"], 2)\n",
    "\n",
    "crn_drb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "Export the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crn_drb.to_parquet(\"s3://nhgf-development/workspace/tutorial/CONUS404/crn_drb.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PRISM data\n",
    "This time we will open the PRISM dataset, temporarally slice it, spatially clip it, and refine the data. Many of the steps will look the same as the CONUS404 dataset  so there will be less explanation of the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = fsspec.filesystem(\"s3\", anon=False, requester_pays=True, skip_instance_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prism_years = range(1979, 2021, 1)\n",
    "chunks={\"time\": 6, \"lon\": 703, \"lat\": 311}\n",
    "pr = [xr.open_dataset(fs.open(f\"s3://nhgf-development/thredds/prism_v2/prism_{str(year)}.nc\"), chunks=chunks, decode_coords=\"all\") for year in prism_years]\n",
    "prism = xr.concat(pr, dim=\"time\")\n",
    "prism = prism.drop_vars(\"time_bnds\")\n",
    "\n",
    "# prism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAD83\n",
    "prism_crs = 4269\n",
    "\n",
    "# write crs to prism\n",
    "prism.rio.write_crs(prism_crs, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename the dimensions to match CF conventions used by rioxarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prism = prism.rename({\"lon\":\"x\", \"lat\":\"y\", \"ppt\": \"PREC_ACC_NC\"}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bring in DRB boundaries and reproject to match PRISM crs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring in boundaries of DRB and create single polygon\n",
    "drb_NAD83 = pygeohydro.WBD(\"huc6\", outfields=[\"huc6\", \"name\"]).byids(\"huc6\", [\"020401\", \"020402\"])\n",
    "# create a column where all entries have the same value\n",
    "drb_NAD83[\"name\"] = \"DRB\"\n",
    "\n",
    "# dissolve by that column\n",
    "drb_NAD83 = drb_NAD83.dissolve(by=\"name\")\n",
    "\n",
    "# set CRS to match ds\n",
    "drb_NAD83 = drb_NAD83.iloc[[0]].to_crs(prism_crs)\n",
    "\n",
    "#visualize\n",
    "# drb_NAD83.plot(edgecolor=\"orange\", facecolor=\"purple\", linewidth=2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clip to DRB\n",
    "prism_drb = prism.rio.clip(drb_NAD83.geometry, crs=prism_crs, drop=True, invert=False)\n",
    "\n",
    "#slice time\n",
    "prism_drb = prism_drb.sel(time=slice(start_date, end_date))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the clipped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prism_drb.sel(time=\"2000-06-01\", method=\"nearest\").hvplot(x='x', y='y', geo=True, rasterize=True, tiles='OSM', alpha=0.7, cmap='turbo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the mean monthly tempertaure and convert to Kelvin and populate its attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean temperature in Kelvin\n",
    "prism_drb = prism_drb.assign(TK = ((prism_drb.tmn+prism_drb.tmx)/2) + 273.15) \n",
    "\n",
    "# dictionary of attributes\n",
    "prism_tk_attrs = {'units': 'K',\n",
    " 'long_name': 'Mean monthly temperature'\n",
    "}\n",
    "\n",
    "# assign attributes\n",
    "prism_drb[\"TK\"] = prism_drb[\"TK\"].assign_attrs(prism_tk_attrs)\n",
    "\n",
    "prism_drb.PREC_ACC_NC.attrs[\"units\"] = 'mm'\n",
    "prism_drb.PREC_ACC_NC.attrs[\"long_name\"] = 'Accumulated grid scale precipitation'\n",
    "\n",
    "# drop variables\n",
    "prism_drb = prism_drb.drop_vars([\"tmn\", \"tmx\"])\n",
    "\n",
    "# drop spatial_ref coord in order to export later\n",
    "prism_drb = prism_drb.reset_coords(\"spatial_ref\", drop=True)\n",
    "\n",
    "# rechunk to make export easier\n",
    "prism_drb = prism_drb.chunk(chunks={\n",
    "    \"y\":98,\n",
    "    \"x\":48,\n",
    "    \"time\":492\n",
    "})\n",
    "\n",
    "# check out final dataset\n",
    "prism_drb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export data as a NetCDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = fsspec.open('simplecache::s3://nhgf-development/workspace/tutorial/CONUS404/prism_drb.nc', \n",
    "                      mode='wb', s3=dict(profile='nhgf-development'))\n",
    "with outfile as f:\n",
    "    prism_drb.load().to_netcdf(f, compute=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NASA's CERES-EBAF Level 3b Dataset\n",
    "The [CERES-EBAF Summary](https://ceres.larc.nasa.gov/documents/DQ_summaries/CERES_EBAF_Ed4.1_DQS.pdf) provides important background information and insights about the data. The [download page](https://ceres.larc.nasa.gov/data/#ebaf-level-3) provides a quicks summary as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring in ceres-ebay\n",
    "ceres = xr.open_dataset(\"https://opendap.larc.nasa.gov/opendap/CERES/EBAF/Edition4.1/CERES_EBAF_Edition4.1_200003-202111.nc\", decode_coords=\"all\")\n",
    "\n",
    "# parse_cf\n",
    "ceres = ceres.metpy.parse_cf()\n",
    "\n",
    "# extract crs\n",
    "ceres_crs = ceres.sfc_net_tot_all_mon.metpy.cartopy_crs\n",
    "\n",
    "# rename\n",
    "ceres = ceres.rename({\"lon\":\"x\", \"lat\":\"y\", \"sfc_net_tot_all_mon\":\"RNET\"}) \n",
    "\n",
    "# x is in 0:360, need it in -180:180\n",
    "ceres = ceres.assign_coords(x=(((ceres.x + 180) % 360) - 180)).sortby('x')\n",
    "\n",
    "# pare down dataset \n",
    "ceres = ceres[\"RNET\"]\n",
    "\n",
    "# set crs\n",
    "ceres.rio.write_crs(ceres_crs, inplace=True)\n",
    "\n",
    "# bring in boundaries of DRB and create single polygon\n",
    "drb_PC = pygeohydro.WBD(\"huc6\", outfields=[\"huc6\", \"name\"]).byids(\"huc6\", [\"020401\", \"020402\"])\n",
    "\n",
    "# create a column where all entries have the same value\n",
    "drb_PC[\"name\"] = \"DRB\"\n",
    "\n",
    "# dissolve by that column\n",
    "drb_PC = drb_PC.dissolve(by=\"name\")\n",
    "\n",
    "# set CRS to match ds\n",
    "drb_PC = drb_PC.iloc[[0]].to_crs(ceres_crs)\n",
    "\n",
    "# clip to DRB\n",
    "ceres_drb = ceres.rio.clip(drb_PC.geometry, crs=ceres_crs, drop=True, invert=False, all_touched=True)\n",
    "\n",
    "#slice time\n",
    "ceres_drb = ceres_drb.sel(time=slice(start_date, end_date))\n",
    "\n",
    "# drop spatial_ref coord in order to export later\n",
    "ceres_drb = ceres_drb.reset_coords([\"spatial_ref\", \"metpy_crs\"], drop=True)\n",
    "\n",
    "#check final dataset\n",
    "ceres_drb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = fsspec.open('simplecache::s3://nhgf-development/workspace/tutorial/CONUS404/ceres_drb.nc', \n",
    "                      mode='wb', s3=dict(profile='nhgf-development'))\n",
    "with outfile as f:\n",
    "    prism_drb.load().to_netcdf(f, compute=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure all files have been created. There should be:\n",
    "1. c404_drb.nc\n",
    "2. ceres_drb.nc\n",
    "3. crn_drb.parquet\n",
    "4. hcn_drb.parquet\n",
    "6. prism_drb.nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = fsspec.filesystem(\"s3\", anon=False, requester_pays=True, skip_instance_cache=True)\n",
    "\n",
    "fs.ls(\"s3://nhgf-development/workspace/tutorial/CONUS404\", detail=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Last code cell of the notebook\n",
    "# import watermark.watermark as watermark\n",
    "# print(watermark(iversions=True, python=True, machine=True, globals_=globals()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close(); cluster.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('hytest')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "6b8b815d206080047d0881750c260f2e84eb4576ca4137e2355fa3de469693c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
