{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create zonal statistics and point extractions for comparing CONUS404 and reference datasets\n",
    "\n",
    "Short paragraph describing what is about to happen\n",
    "\n",
    "<details>\n",
    "  <summary>Guide to pre-requisites and learning outcomes...&lt;click to expand&gt;</summary>\n",
    "  \n",
    "  <table>\n",
    "    <tr>\n",
    "      <td>Pre-Requisites\n",
    "      <td>To get the most out of this notebook, you should already have an understanding of these topics: \n",
    "        <ul>\n",
    "        <li>pre-req one\n",
    "        <li>pre-req two\n",
    "        </ul>\n",
    "    <tr>\n",
    "      <td>Expected Results\n",
    "      <td>At the end of this notebook, you should be able to: \n",
    "        <ul>\n",
    "        <li>outcome one\n",
    "        <li>outcome two\n",
    "        </ul>\n",
    "  </table>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library imports\n",
    "import fsspec #testing\n",
    "import hvplot.xarray #testing\n",
    "import intake #testing\n",
    "import os #testing\n",
    "import warnings #testing\n",
    "import rioxarray #testing\n",
    "import dask #testing\n",
    "import metpy #testing\n",
    "import calendar #testing\n",
    "\n",
    "from shapely.geometry import Polygon #testing\n",
    "from dask.distributed import LocalCluster, Client #testing\n",
    "from pygeohydro import pygeohydro #testing\n",
    "from fsspec.implementations.ftp import FTPFileSystem #testing\n",
    "from holoviews.streams import PolyEdit, PolyDraw #testing\n",
    "from geocube.api.core import make_geocube #testing\n",
    "\n",
    "import xarray as xr #testing\n",
    "import geopandas as gpd #testing\n",
    "import pandas as pd #testing\n",
    "import geoviews as gv #testing\n",
    "import dask.dataframe as dd #testing\n",
    "import numpy as np #testing\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update to helper function after repo consolidation\n",
    "## **Start a Dask client using an appropriate Dask Cluster** \n",
    "This is an optional step, but can speed up data loading significantly, especially when accessing data from the cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_cluster(machine):\n",
    "    ''' Helper function to configure cluster\n",
    "    '''\n",
    "    if machine == 'denali':\n",
    "        from dask.distributed import LocalCluster, Client\n",
    "        cluster = LocalCluster(threads_per_worker=1)\n",
    "        client = Client(cluster)\n",
    "    \n",
    "    elif machine == 'tallgrass':\n",
    "        from dask.distributed import Client\n",
    "        from dask_jobqueue import SLURMCluster\n",
    "        cluster = SLURMCluster(queue='cpu', cores=1, interface='ib0',\n",
    "                               job_extra=['--nodes=1', '--ntasks-per-node=1', '--cpus-per-task=1'],\n",
    "                               memory='6GB')\n",
    "        cluster.adapt(maximum_jobs=30)\n",
    "        client = Client(cluster)\n",
    "        \n",
    "    elif machine == 'local':\n",
    "        import os\n",
    "        import warnings\n",
    "        from dask.distributed import LocalCluster, Client\n",
    "        warnings.warn(\"Running locally can result in costly data transfers!\\n\")\n",
    "        n_cores = os.cpu_count() # set to match your machine\n",
    "        cluster = LocalCluster(threads_per_worker=n_cores)\n",
    "        client = Client(cluster)\n",
    "        \n",
    "    elif machine in ['esip-qhub-gateway-v0.4']:   \n",
    "        import sys, os\n",
    "        sys.path.append(os.path.join(os.environ['HOME'],'shared','users','lib'))\n",
    "        import ebdpy as ebd\n",
    "        aws_profile = 'nhgf-development'\n",
    "        ebd.set_credentials(profile=aws_profile)\n",
    "\n",
    "        aws_region = 'us-west-2'\n",
    "        endpoint = f's3.{aws_region}.amazonaws.com'\n",
    "        ebd.set_credentials(profile=aws_profile, region=aws_region, endpoint=endpoint)\n",
    "        worker_max = 30\n",
    "        client,cluster = ebd.start_dask_cluster(profile=aws_profile, worker_max=worker_max, \n",
    "                                              region=aws_region, use_existing_cluster=True,\n",
    "                                              adaptive_scaling=False, wait_for_cluster=False, \n",
    "                                              worker_profile='Medium Worker', propagate_env=True)\n",
    "        \n",
    "    return client, cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup your cluster\n",
    "\n",
    "#### QHub...\n",
    "Uncomment single commented spaces (#) to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set machine\n",
    "machine = 'esip-qhub-gateway-v0.4'\n",
    "\n",
    "# use configure cluster helper function to setup dask\n",
    "client, cluster = configure_cluster(machine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### or HPC\n",
    "Uncomment single commented spaces (#) to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set machine\n",
    "# machine = os.environ['SLURM_CLUSTER_NAME']\n",
    "\n",
    "## use configure_cluster helper function to setup dask\n",
    "# client, cluster = configure_cluster(machine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Compute zonal statistics for gridded datasets**\n",
    "\n",
    "In the last tutorial, we prepared three gridded datasets: CONUS404 (benchmark), PRISM (reference), and CERES-EBAF (reference). The goal of this section is compute [zonal statistics](https://gisgeography.com/zonal-statistics/) for each HUC6 zone in the Delaware River Basin (DRB) at each time-step in the data. This tabular data will then be exported for use in the next notebook, **CONUS404 Analysis**.\n",
    "\n",
    "Dataset outline:\n",
    "1. Read in the prepared dataset\n",
    "2. Read in the HUC6 boundaries and transform to same coordinate reference system as prepared dataset\n",
    "3. Make a data mask with the HUC6 boundaries to calculate zonal statistics\n",
    "4. Compute zonal statistics with data mask and prepared data\n",
    "\n",
    "Once all calculations are done: \n",
    "\n",
    "5. Combine each reference with benchmark into single dataset\n",
    "6. Export gridded data zonal statistics\n",
    "<br>\n",
    "\n",
    "**CONUS404 zonal statistics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# url to c404_drb\n",
    "c404_drb_url = 's3://nhgf-development/workspace/tutorial/CONUS404/c404_drb.nc'\n",
    "\n",
    "fs = fsspec.filesystem(\"s3\", anon=False, requester_pays=True, skip_instance_cache=True)\n",
    "\n",
    "# open dataset\n",
    "c404_drb = xr.open_dataset(fs.open(c404_drb_url), decode_coords=\"all\")\n",
    "\n",
    "# set crs\n",
    "c404_crs = c404_drb.rio.crs.to_proj4()\n",
    "\n",
    "c404_drb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in HUC6 boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# bring in HUC6 boundaries found in the DRB\n",
    "c404_drb_gdf = pygeohydro.WBD(\"huc6\", outfields=[\"huc6\", \"name\"]).byids(\"huc6\", [\"020401\", \"020402\"])\n",
    "\n",
    "# set CRS to match c404_drb\n",
    "c404_drb_gdf = c404_drb_gdf.to_crs(c404_crs)\n",
    "\n",
    "#visualize\n",
    "# c404_drb_gdf.plot(edgecolor=\"orange\", facecolor=\"purple\", linewidth=2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing geocube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create datamask and build new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert huc6 field to int as this works best for the following steps\n",
    "c404_drb_gdf[\"huc6\"] = c404_drb_gdf[\"huc6\"].astype(int) #note: this may drop the # of digits from 6 to less depending on how many zeroes there were, may need to pad back to 6 digits later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c404_drb.rio.write_crs(c404_crs, inplace=True) \n",
    "\n",
    "# create an output grid\n",
    "c404_out_grid = make_geocube(\n",
    "    vector_data = c404_drb_gdf,\n",
    "    measurements=[\"huc6\"],\n",
    "    like=c404_drb\n",
    ")\n",
    "\n",
    "# add datarrays to grid\n",
    "c404_out_grid[\"RNET\"] = (c404_drb.RNET.dims, c404_drb.RNET.values, \n",
    "                         c404_drb.RNET.attrs, c404_drb.RNET.encoding)\n",
    "\n",
    "c404_out_grid[\"TK\"] = (c404_drb.TK.dims, c404_drb.TK.values,\n",
    "                         c404_drb.TK.attrs, c404_drb.TK.encoding)\n",
    "\n",
    "c404_out_grid[\"PREC_ACC_NC\"] = (c404_drb.PREC_ACC_NC.dims, c404_drb.PREC_ACC_NC.values,\n",
    "                         c404_drb.PREC_ACC_NC.attrs, c404_drb.PREC_ACC_NC.encoding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group data arrays by HUC6 code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_grouped = c404_out_grid.drop_vars(\"spatial_ref\").groupby(c404_out_grid.huc6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the mean variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_grid_mean = c404_grouped.mean().rename({\"RNET\": \"c404_RNET_mean\", \"TK\": \"c404_TK_mean\", \n",
    "                                       \"PREC_ACC_NC\": \"c404_PREC_ACC_NC_mean\", \"time\":\"time_index\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_zonal_stats = c404_grid_mean.to_dataframe().drop(\"spatial_ref\", axis=1)\n",
    "c404_zonal_stats.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time has been replaced by the position index from the *c404_drb* time coordinate. Ungroup the data and add the time value from the index to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_zonal_stats = c404_zonal_stats.reset_index(drop=False)\n",
    "\n",
    "c404_zonal_stats[\"time\"] = c404_drb.coords[\"time\"][c404_zonal_stats[\"time_index\"].values]\n",
    "\n",
    "c404_zonal_stats.drop(\"time_index\", axis=1, inplace=True)\n",
    "\n",
    "c404_zonal_stats[\"time\"] = c404_zonal_stats[\"time\"].astype(str).str[:-3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset huc6 back to a string type of length 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_zonal_stats[\"huc6\"] = c404_zonal_stats[\"huc6\"].astype(int).astype(str).str.zfill(6) # pads with 0's to make all column values lenght == 0\n",
    "c404_zonal_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PRISM zonal statistics**\n",
    "\n",
    "PRISM has two variables: TK and PREC_ACC_NC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url to prism_drb\n",
    "prism_drb_url = 's3://nhgf-development/workspace/tutorial/CONUS404/prism_drb.nc'\n",
    "\n",
    "fs = fsspec.filesystem(\"s3\", anon=False, requester_pays=True, skip_instance_cache=True)\n",
    "\n",
    "# open dataset\n",
    "prism_drb = xr.open_dataset(fs.open(prism_drb_url), decode_coords=\"all\")\n",
    "\n",
    "# set crs\n",
    "prism_crs = prism_drb.rio.crs.to_proj4()\n",
    "\n",
    "# bring in HUC6 boundaries found in the DRB\n",
    "prism_drb_gdf = pygeohydro.WBD(\"huc6\", outfields=[\"huc6\", \"name\"]).byids(\"huc6\", [\"020401\", \"020402\"])\n",
    "\n",
    "# set CRS to match prism_drb\n",
    "prism_drb_gdf = prism_drb_gdf.to_crs(prism_crs)\n",
    "\n",
    "# convert huc6 field to int as this works best for the following steps\n",
    "prism_drb_gdf[\"huc6\"] = prism_drb_gdf[\"huc6\"].astype(int) #note: this may drop the # of digits from 6 to less depending on how many zeroes \n",
    "                                                            #  there were, may need to pad back to 6 digits later\n",
    "\n",
    "# create an output grid\n",
    "prism_out_grid = make_geocube(\n",
    "    vector_data = prism_drb_gdf,\n",
    "    measurements=[\"huc6\"],\n",
    "    like=prism_drb\n",
    ")\n",
    "\n",
    "# add datarrays to grid\n",
    "prism_out_grid[\"TK\"] = (prism_drb.TK.dims, prism_drb.TK.values,\n",
    "                         prism_drb.TK.attrs, prism_drb.TK.encoding)\n",
    "\n",
    "prism_out_grid[\"PREC_ACC_NC\"] = (prism_drb.PREC_ACC_NC.dims, prism_drb.PREC_ACC_NC.values,\n",
    "                         prism_drb.PREC_ACC_NC.attrs, prism_drb.PREC_ACC_NC.encoding)\n",
    "\n",
    "# groupby\n",
    "prism_grouped = prism_out_grid.drop_vars(\"spatial_ref\").groupby(prism_out_grid.huc6)\n",
    "\n",
    "# Calculate the mean variables\n",
    "prism_grid_mean = prism_grouped.mean().rename({\"TK\": \"prism_TK_mean\", \n",
    "                                       \"PREC_ACC_NC\": \"prism_PREC_ACC_NC_mean\", \"time\":\"time_index\"})\n",
    "\n",
    "#convert to a dataframe\n",
    "prism_zonal_stats = prism_grid_mean.to_dataframe().drop(\"spatial_ref\", axis=1)\n",
    "\n",
    "# reste index and add time back\n",
    "prism_zonal_stats = prism_zonal_stats.reset_index(drop=False)\n",
    "prism_zonal_stats[\"time\"] = prism_drb.coords[\"time\"][prism_zonal_stats[\"time_index\"].values]\n",
    "prism_zonal_stats.drop(\"time_index\", axis=1, inplace=True)\n",
    "prism_zonal_stats[\"time\"] = prism_zonal_stats[\"time\"].astype(str).str[:-3]\n",
    "\n",
    "# change huc6 to string and pad with zeros\n",
    "prism_zonal_stats[\"huc6\"] = prism_zonal_stats[\"huc6\"].astype(int).astype(str).str.zfill(6) # pads with 0's to make all column values lenght == 0\n",
    "\n",
    "prism_zonal_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the PRISM and CONUS404 zonals stats together based on the HUC6 code and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prism_c404_zonal = prism_zonal_stats.merge(c404_zonal_stats, left_on=['huc6', 'time'], right_on=['huc6', 'time'])\n",
    "prism_c404_zonal.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need the CONUS404 RNET value so we'll drop that column before exporting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prism_c404_zonal.drop(\"c404_RNET_mean\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prism_c404_zonal.to_parquet(\"s3://nhgf-development/workspace/tutorial/CONUS404/prism_c404_zonal.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CERES-EBAF zonal statistics**\n",
    "\n",
    "CERES-EBAF has a single variable: RNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url to ceres_drb\n",
    "ceres_drb_url = 's3://nhgf-development/workspace/tutorial/CONUS404/ceres_drb.nc'\n",
    "\n",
    "fs = fsspec.filesystem(\"s3\", anon=False, requester_pays=True, skip_instance_cache=True)\n",
    "\n",
    "# open dataset\n",
    "ceres_drb = xr.open_dataset(fs.open(ceres_drb_url), decode_coords=\"all\")\n",
    "\n",
    "# set crs\n",
    "ceres_crs = ceres_drb.rio.crs.to_proj4()\n",
    "\n",
    "# bring in HUC6 boundaries found in the DRB\n",
    "ceres_drb_gdf = pygeohydro.WBD(\"huc6\", outfields=[\"huc6\", \"name\"]).byids(\"huc6\", [\"020401\", \"020402\"])\n",
    "\n",
    "# set CRS to match ceres_drb\n",
    "ceres_drb_gdf = ceres_drb_gdf.to_crs(ceres_crs)\n",
    "\n",
    "# convert huc6 field to int as this works best for the following steps\n",
    "ceres_drb_gdf[\"huc6\"] = ceres_drb_gdf[\"huc6\"].astype(int) #note: this may drop the # of digits from 6 to less depending on how many zeroes \n",
    "                                                            #  there were, may need to pad back to 6 digits later\n",
    "    \n",
    "# create an output grid\n",
    "ceres_out_grid = make_geocube(\n",
    "    vector_data = ceres_drb_gdf,\n",
    "    measurements=[\"huc6\"],\n",
    "    like=ceres_drb\n",
    ")\n",
    "\n",
    "# add datarrays to grid\n",
    "ceres_out_grid[\"RNET\"] = (ceres_drb.RNET.dims, ceres_drb.RNET.values,\n",
    "                         ceres_drb.RNET.attrs, ceres_drb.RNET.encoding)\n",
    "\n",
    "# groupby\n",
    "ceres_grouped = ceres_out_grid.drop_vars(\"spatial_ref\").groupby(ceres_out_grid.huc6)\n",
    "\n",
    "# Calculate the mean variables\n",
    "ceres_grid_mean = ceres_grouped.mean().rename({\"RNET\": \"ceres_RNET_mean\", \"time\":\"time_index\"})\n",
    "\n",
    "#convert to a dataframe\n",
    "ceres_zonal_stats = ceres_grid_mean.to_dataframe().drop(\"spatial_ref\", axis=1)\n",
    "\n",
    "# reste index and add time back\n",
    "ceres_zonal_stats = ceres_zonal_stats.reset_index(drop=False)\n",
    "ceres_zonal_stats[\"time\"] = ceres_drb.coords[\"time\"][ceres_zonal_stats[\"time_index\"].values]\n",
    "ceres_zonal_stats.drop(\"time_index\", axis=1, inplace=True)\n",
    "ceres_zonal_stats[\"time\"] = ceres_zonal_stats[\"time\"].astype(str).str[:-3]\n",
    "\n",
    "# change huc6 to string and pad with zeros\n",
    "ceres_zonal_stats[\"huc6\"] = ceres_zonal_stats[\"huc6\"].astype(int).astype(str).str.zfill(6) # pads with 0's to make all column values lenght == 0\n",
    "\n",
    "ceres_zonal_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the CERES-EBAF and CONUS404 zonals stats together based on the HUC6 code and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceres_c404_zonal = ceres_zonal_stats.merge(c404_zonal_stats, left_on=['huc6', 'time'], right_on=['huc6', 'time'])\n",
    "ceres_c404_zonal.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need the CONUS404 TK and PREC_ACC_NC values so we'll drop these columns before exporting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceres_c404_zonal.drop([\"c404_TK_mean\", \"c404_PREC_ACC_NC_mean\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceres_c404_zonal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ceres_c404_zonal.to_parquet(\"s3://nhgf-development/workspace/tutorial/CONUS404/ceres_c404_zonal.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Extract gridded values to points**\n",
    "\n",
    "The goal of this section is extract values from CONUS404 where they intersect with station data. This process is described in article about the ESRI tool [Extract Values to Points](https://pro.arcgis.com/en/pro-app/latest/tool-reference/spatial-analyst/extract-values-to-points.htm). This tabular data will then be exported for use in the next notebook, **CONUS404 Analysis**.\n",
    "\n",
    "Dataset outline:\n",
    "1. Read in the prepared dataset\n",
    "2. Extract data from overlapping pixel at same time step as point\n",
    "<br>\n",
    "\n",
    "**Climate Reference Network point extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = fsspec.filesystem(\"s3\", anon=False, requester_pays=True, skip_instance_cache=True)\n",
    "\n",
    "crn_drb_df = pd.read_parquet(fs.open(\"s3://nhgf-development/workspace/tutorial/CONUS404/crn_drb.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crn_drb = gpd.GeoDataFrame(crn_drb_df, \n",
    "                       geometry=gpd.points_from_xy(crn_drb_df.LONGITUDE, \n",
    "                                                         crn_drb_df.LATITUDE))\n",
    "\n",
    "# modify date field\n",
    "crn_drb[\"DATE\"] = crn_drb[\"DATE\"].astype(str).str[:-3]\n",
    "\n",
    "crn_drb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure all files have been created. There should be:\n",
    "1. c404_drb.nc\n",
    "2. ceres_drb.nc\n",
    "3. crn_drb.parquet\n",
    "4. hcn_drb.parquet\n",
    "6. prism_drb.nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = fsspec.filesystem(\"s3\", anon=False, requester_pays=True, skip_instance_cache=True)\n",
    "\n",
    "fs.ls(\"s3://nhgf-development/workspace/tutorial/CONUS404\", detail=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Last code cell of the notebook\n",
    "# import watermark.watermark as watermark\n",
    "# print(watermark(iversions=True, python=True, machine=True, globals_=globals()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close(); cluster.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "users-pangeo",
   "language": "python",
   "name": "conda-env-users-pangeo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "6b8b815d206080047d0881750c260f2e84eb4576ca4137e2355fa3de469693c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
