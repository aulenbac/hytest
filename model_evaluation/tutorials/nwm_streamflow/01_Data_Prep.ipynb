{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation :: Data Preparation\n",
    "\n",
    "As a part of the generalized evaluation workflow: \n",
    "\n",
    "<img src='./Eval_PreProc.svg' width=600>\n",
    "\n",
    "The pre-processing step is needed in order to align the two datasets for analysis.  The specific \n",
    "steps needed to prepare a given dataset may differ, depending on the source and the variable of\n",
    "interest. \n",
    "\n",
    "Some steps might include: \n",
    "\n",
    "* Organizing the time-series index such that the time steps for both simulated and observed are congruent\n",
    "    * This may involve interpolation to estimate a more granular time-step than is found in the source data\n",
    "    * More often, an agregating function is used to 'down-sample' the dataset to a coarser time step (days vs hours).\n",
    "* Coordinate aggregation units between simulated and observed \n",
    "    * Gridded data may be sampled per HUC-12, HUC-6, etc. to match modeled data indexed by these units. \n",
    "    * Index formats may be adjusted (e.g. a 'gage_id' may be 'USGS-01104200' in one data set, vs '01104200' in another)\n",
    "* Re-Chunking the data to make time-series analysis more efficient (see [here](/dev/null) for a primer on re-chunking).\n",
    "\n",
    "At this stage, a given variable should be represented as a pair of 2D array of values (one for simulated, one for observed). \n",
    "One dimension of the array is indexed by some nominal data field (e.g. 'gage_id', 'HUC-12 ID', etc), while the other dimension \n",
    "is indexed by time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ds_chanobs = xr.open_dataset(fs2.get_mapper(url), engine='zarr', \n",
    "                             backend_kwargs={'consolidated':False}, chunks={})\n",
    "\n",
    "gage_ids_str = [gage_id.astype('str').lstrip() for gage_id in ds_chanobs['gage_id'].values]\n",
    "## what the gage IDs look like:\n",
    "gage_ids_str[0:5]\n",
    "# determine the start and end of the modeled timeseries\n",
    "start = ds_chanobs.time[0].values\n",
    "stop = ds_chanobs.time[-1].values\n",
    "print(start,stop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pygeohydro\n",
    "from pygeohydro import NWIS\n",
    "nwis = NWIS()\n",
    "# use the start and stop dates above from the modeled data to extract observational data from NWIS for the same time period\n",
    "dates = (start,stop)\n",
    "print(dates)\n",
    "%%time\n",
    "ds_obs = nwis.get_streamflow(gage_ids_str[:2], dates, to_xarray=True)\n",
    "# rename variables\n",
    "ds_obs = ds_obs.rename_dims({'station_id':'gage_id'}).rename({'station_id':'gage_id','discharge':'streamflow'})\n",
    "time_base = ds_obs.time.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit this to your directory where you wish to save NWIS streamflow information\n",
    "dir_scratch = Path('/caldera/projects/usgs/water/wbbp/')\n",
    "file_chanobs = dir_scratch / 'nwis_chanobs2.zarr'\n",
    "if file_chanobs.is_dir():\n",
    "    fs.rm(str(file_chanobs),recursive=True)\n",
    "len(gage_ids_str)\n",
    "#source_dataset = ds_obs.drop_vars(drop_vars)\n",
    "source_dataset = ds_obs\n",
    "template = (source_dataset.chunk().\n",
    "            pipe(xr.zeros_like).\n",
    "            isel(gage_id=0, drop=True).\n",
    "            expand_dims(gage_id=len(gage_ids_str), axis=-1))\n",
    "\n",
    "template = template.assign_coords({'gage_id':[f'USGS-{gage_id}' for gage_id in gage_ids_str]})\n",
    "\n",
    "template = template.chunk({'time':len(ds_obs.time), 'gage_id': 1})\n",
    "\n",
    "encoding = {'alt_acy_va': dict(_FillValue=-2147483647, dtype=np.int32),\n",
    "            'alt_va': dict( _FillValue=9.96921e+36, dtype=np.float32),\n",
    "            'dec_lat_va': dict( _FillValue=None, dtype=np.float32),\n",
    "            'dec_long_va': dict( _FillValue=None, dtype=np.float32),\n",
    "            'streamflow': dict( _FillValue=9.96921e+36, dtype=np.float32)}\n",
    "template.to_zarr(file_chanobs, compute=False, encoding=encoding, consolidated=True, mode='w')\n",
    "nt = len(ds_obs.time)\n",
    "ds_obs.to_zarr(file_chanobs, region={'time':slice(0, nt), 'gage_id': slice(0, 2)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ind2zarr(n):\n",
    "     site_id = gage_ids_str[n]\n",
    "     try:\n",
    "        ds_obs = nwis.get_streamflow(site_id, dates, to_xarray=True).interp(time=time_base)\n",
    "        ds_obs = ds_obs.rename_dims({'station_id':'gage_id'}).rename({'station_id':'gage_id','discharge':'streamflow'})\n",
    "        ds_obs.to_zarr(file_chanobs, region={'time': slice(0, nt), 'gage_id': slice(n,n+1)})\n",
    "     except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client, cluster = configure_cluster(resource)\n",
    "_ = dask.compute(*[dask.delayed(ind2zarr)(i) for i in range(len(gage_ids_str))], retries=10);\n",
    "_ = consolidate_metadata(file_chanobs)\n",
    "file_chanobs\n",
    "dst = xr.open_dataset(file_chanobs, engine='zarr', chunks={}, backend_kwargs=dict(consolidated=True))\n",
    "dst"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "d7ebce313f85fb1ac8949e834c83f371584cb2422d845bf1570c1220fdedc716"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
