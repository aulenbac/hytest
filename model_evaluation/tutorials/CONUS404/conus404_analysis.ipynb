{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparative analysis of CONUS404 to reference datasets\n",
    "\n",
    "<img src='./Eval_Analysis.svg' width=600>\n",
    "\n",
    "With our "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library imports\n",
    "import fsspec #testing\n",
    "import hvplot.xarray #testing\n",
    "import intake #testing\n",
    "import os #testing\n",
    "import warnings #testing\n",
    "import rioxarray #testing\n",
    "import dask #testing\n",
    "import metpy #testing\n",
    "import calendar #testing\n",
    "\n",
    "from shapely.geometry import Polygon #testing\n",
    "from dask.distributed import LocalCluster, Client #testing\n",
    "from pygeohydro import pygeohydro #testing\n",
    "from fsspec.implementations.ftp import FTPFileSystem #testing\n",
    "from holoviews.streams import PolyEdit, PolyDraw #testing\n",
    "from geocube.api.core import make_geocube #testing\n",
    "\n",
    "import xarray as xr #testing\n",
    "import geopandas as gpd #testing\n",
    "import pandas as pd #testing\n",
    "import geoviews as gv #testing\n",
    "import dask.dataframe as dd #testing\n",
    "import numpy as np #testing  \n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update to helper function after repo consolidation\n",
    "## **Start a Dask client using an appropriate Dask Cluster** \n",
    "This is an optional step, but can speed up data loading significantly, especially when accessing data from the cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_cluster(machine):\n",
    "    ''' Helper function to configure cluster\n",
    "    '''\n",
    "    if machine == 'denali':\n",
    "        from dask.distributed import LocalCluster, Client\n",
    "        cluster = LocalCluster(threads_per_worker=1)\n",
    "        client = Client(cluster)\n",
    "    \n",
    "    elif machine == 'tallgrass':\n",
    "        from dask.distributed import Client\n",
    "        from dask_jobqueue import SLURMCluster\n",
    "        cluster = SLURMCluster(queue='cpu', cores=1, interface='ib0',\n",
    "                               job_extra=['--nodes=1', '--ntasks-per-node=1', '--cpus-per-task=1'],\n",
    "                               memory='6GB')\n",
    "        cluster.adapt(maximum_jobs=30)\n",
    "        client = Client(cluster)\n",
    "        \n",
    "    elif machine == 'local':\n",
    "        import os\n",
    "        import warnings\n",
    "        from dask.distributed import LocalCluster, Client\n",
    "        warnings.warn(\"Running locally can result in costly data transfers!\\n\")\n",
    "        n_cores = os.cpu_count() # set to match your machine\n",
    "        cluster = LocalCluster(threads_per_worker=n_cores)\n",
    "        client = Client(cluster)\n",
    "        \n",
    "    elif machine in ['esip-qhub-gateway-v0.4']:   \n",
    "        import sys, os\n",
    "        sys.path.append(os.path.join(os.environ['HOME'],'shared','users','lib'))\n",
    "        import ebdpy as ebd\n",
    "        aws_profile = 'nhgf-development'\n",
    "        ebd.set_credentials(profile=aws_profile)\n",
    "\n",
    "        aws_region = 'us-west-2'\n",
    "        endpoint = f's3.{aws_region}.amazonaws.com'\n",
    "        ebd.set_credentials(profile=aws_profile, region=aws_region, endpoint=endpoint)\n",
    "        worker_max = 30\n",
    "        client,cluster = ebd.start_dask_cluster(profile=aws_profile, worker_max=worker_max, \n",
    "                                              region=aws_region, use_existing_cluster=True,\n",
    "                                              adaptive_scaling=False, wait_for_cluster=False, \n",
    "                                              worker_profile='Medium Worker', propagate_env=True)\n",
    "        \n",
    "    return client, cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup your cluster\n",
    "\n",
    "#### QHub...\n",
    "Uncomment single commented spaces (#) to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set machine\n",
    "machine = 'esip-qhub-gateway-v0.4'\n",
    "\n",
    "# use configure cluster helper function to setup dask\n",
    "client, cluster = configure_cluster(machine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### or HPC\n",
    "Uncomment single commented spaces (#) to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set machine\n",
    "# machine = os.environ['SLURM_CLUSTER_NAME']\n",
    "\n",
    "## use configure_cluster helper function to setup dask\n",
    "# client, cluster = configure_cluster(machine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate weighted annual means\n",
    "\n",
    "Create a function to calculate weighted annual means\n",
    "\n",
    "Credit: [NCAR](https://ncar.github.io/esds/posts/2021/yearly-averages-xarray/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_temporal_mean(ds, var):\n",
    "    \"\"\"\n",
    "    weight by days in each month\n",
    "    \"\"\"\n",
    "    # Determine the month length\n",
    "    month_length = ds.time.dt.days_in_month\n",
    "\n",
    "    # Calculate the weights\n",
    "    wgts = month_length.groupby(\"time.year\") / month_length.groupby(\"time.year\").sum()\n",
    "\n",
    "    # Make sure the weights in each year add up to 1\n",
    "    np.testing.assert_allclose(wgts.groupby(\"time.year\").sum(xr.ALL_DIMS), 1.0)\n",
    "\n",
    "    # Subset our dataset for our variable\n",
    "    obs = ds[var]\n",
    "\n",
    "    # Setup our masking for nan values\n",
    "    cond = obs.isnull()\n",
    "    ones = xr.where(cond, 0.0, 1.0)\n",
    "\n",
    "    # Calculate the numerator\n",
    "    obs_sum = (obs * wgts).resample(time=\"AS\").sum(dim=\"time\")\n",
    "\n",
    "    # Calculate the denominator\n",
    "    ones_out = (ones * wgts).resample(time=\"AS\").sum(dim=\"time\")\n",
    "\n",
    "    # Return the weighted average\n",
    "    return obs_sum / ones_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CONUS404**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# url to c404_drb\n",
    "c404_drb_url = 's3://nhgf-development/workspace/tutorial/CONUS404/c404_drb.nc'\n",
    "\n",
    "fs = fsspec.filesystem(\"s3\", anon=False, requester_pays=True, skip_instance_cache=True)\n",
    "\n",
    "# open dataset\n",
    "c404_drb = xr.open_dataset(fs.open(c404_drb_url), decode_coords=\"all\")\n",
    "\n",
    "# set crs\n",
    "c404_crs = c404_drb.rio.crs.to_proj4()\n",
    "\n",
    "# see what data variables ds has\n",
    "print(c404_drb.data_vars)\n",
    "\n",
    "# see what coordss ds has\n",
    "print(c404_drb.coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_RNET = weighted_temporal_mean(c404_drb, \"RNET\").assign_attrs(c404_drb.RNET.attrs)\n",
    "c404_PREC_ACC_NC = weighted_temporal_mean(c404_drb, \"PREC_ACC_NC\").assign_attrs(c404_drb.PREC_ACC_NC.attrs)\n",
    "c404_TK = weighted_temporal_mean(c404_drb, \"TK\").assign_attrs(c404_drb.TK.attrs)\n",
    "\n",
    "c404_drb_yr = c404_RNET.to_dataset(name=\"RNET\")\n",
    "c404_drb_yr = c404_drb_yr.assign(PREC_ACC_NC = c404_PREC_ACC_NC, TK=c404_TK) \\\n",
    "                        .assign_attrs(c404_drb.attrs) \\\n",
    "                        .assign_coords({\"crs\":c404_drb.coords[\"crs\"]})\n",
    "c404_drb_yr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PRISM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url to prism_drb\n",
    "prism_drb_url = 's3://nhgf-development/workspace/tutorial/CONUS404/prism_drb.nc'\n",
    "\n",
    "fs = fsspec.filesystem(\"s3\", anon=False, requester_pays=True, skip_instance_cache=True)\n",
    "\n",
    "# open dataset\n",
    "prism_drb = xr.open_dataset(fs.open(prism_drb_url), decode_coords=\"all\")\n",
    "\n",
    "# see what data variables ds has\n",
    "print(prism_drb.data_vars)\n",
    "\n",
    "# see what coordss ds has\n",
    "print(prism_drb.coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prism_PREC_ACC_NC = weighted_temporal_mean(prism_drb, \"PREC_ACC_NC\").assign_attrs(prism_drb.PREC_ACC_NC.attrs)\n",
    "prism_TK = weighted_temporal_mean(prism_drb, \"TK\").assign_attrs(prism_drb.TK.attrs)\n",
    "\n",
    "prism_drb_yr = prism_PREC_ACC_NC.to_dataset(name=\"PREC_ACC_NC\")\n",
    "prism_drb_yr = prism_drb_yr.assign(TK=prism_TK) \\\n",
    "                        .assign_attrs(prism_drb.attrs) \\\n",
    "                        .assign_coords({\"spatial_ref\":prism_drb.coords[\"spatial_ref\"]})\n",
    "prism_drb_yr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CERES-EBAF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url to ceres_drb\n",
    "ceres_drb_url = 's3://nhgf-development/workspace/tutorial/CONUS404/ceres_drb.nc'\n",
    "\n",
    "fs = fsspec.filesystem(\"s3\", anon=False, requester_pays=True, skip_instance_cache=True)\n",
    "\n",
    "# open dataset\n",
    "ceres_drb = xr.open_dataset(fs.open(ceres_drb_url), decode_coords=\"all\", chunks={\"time\":10})\n",
    "\n",
    "# see what data variables ds has\n",
    "print(ceres_drb.data_vars)\n",
    "\n",
    "# see what coordss ds has\n",
    "print(ceres_drb.coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceres_RNET = weighted_temporal_mean(ceres_drb, \"RNET\").assign_attrs(ceres_drb.RNET.attrs)\n",
    "\n",
    "ceres_drb_yr = ceres_RNET.to_dataset(name=\"RNET\")\n",
    "ceres_drb_yr = ceres_drb_yr.assign_attrs(ceres_drb.attrs) \\\n",
    "                        .assign_coords({\"spatial_ref\":ceres_drb.coords[\"spatial_ref\"]})\n",
    "ceres_drb_yr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate long-term means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "c404_drb_ltm = c404_drb_yr.mean(dim=\"time\")\n",
    "# c404_drb_ltm = c404_drb_ltm.persist()\n",
    "\n",
    "prism_drb_ltm = prism_drb_yr.mean(dim=\"time\")\n",
    "# prism_drb_ltm = prism_drb_ltm.persist()\n",
    "\n",
    "ceres_drb_ltm = ceres_drb_yr.mean(dim=\"time\")\n",
    "# ceres_drb_ltm = ceres_drb_ltm.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up geographic areas for any zonal statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in HUC6 boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# bring in HUC6 boundaries found in the DRB\n",
    "drb_gdf = pygeohydro.WBD(\"huc6\", outfields=[\"huc6\", \"name\"]).byids(\"huc6\", [\"020401\", \"020402\"])\n",
    "\n",
    "# set CRS to match c404_drb\n",
    "drb_gdf = drb_gdf.to_crs(c404_crs)\n",
    "\n",
    "# convert huc6 field to int as this works best for the following steps\n",
    "drb_gdf[\"huc6\"] = drb_gdf[\"huc6\"].astype(int) #note: this may drop the # of digits from 6 to less depending on how many zeroes there were, may need to pad back to 6 digits later\n",
    "\n",
    "#visualize\n",
    "# drb_gdf.plot(edgecolor=\"orange\", facecolor=\"purple\", linewidth=2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create datamask and build new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CONUS404\n",
    "\n",
    "# create an output grid\n",
    "c404_out_grid = make_geocube(\n",
    "    vector_data = drb_gdf,\n",
    "    measurements=[\"huc6\"],\n",
    "    like=c404_drb_ltm.rio.write_crs(c404_crs)\n",
    ")\n",
    "\n",
    "# add datarrays to grid\n",
    "c404_out_grid[\"RNET\"] = (c404_drb_ltm.RNET.dims, c404_drb_ltm.RNET.values, \n",
    "                         c404_drb_ltm.RNET.attrs, c404_drb_ltm.RNET.encoding)\n",
    "\n",
    "c404_out_grid[\"TK\"] = (c404_drb_ltm.TK.dims, c404_drb_ltm.TK.values,\n",
    "                         c404_drb_ltm.TK.attrs, c404_drb_ltm.TK.encoding)\n",
    "\n",
    "c404_out_grid[\"PREC_ACC_NC\"] = (c404_drb_ltm.PREC_ACC_NC.dims, c404_drb_ltm.PREC_ACC_NC.values,\n",
    "                         c404_drb_ltm.PREC_ACC_NC.attrs, c404_drb_ltm.PREC_ACC_NC.encoding)\n",
    "\n",
    "\n",
    "# c404_grouped = c404_out_grid.drop_vars(\"spatial_ref\").groupby(c404_out_grid.huc6)\n",
    "\n",
    "# PRISM\n",
    "\n",
    "# create an output grid\n",
    "prism_out_grid = make_geocube(\n",
    "    vector_data = drb_gdf,\n",
    "    measurements=[\"huc6\"],\n",
    "    like=prism_drb_ltm.rio.write_crs(c404_crs)\n",
    ")\n",
    "\n",
    "# add datarrays to grid\n",
    "prism_out_grid[\"TK\"] = (prism_drb_ltm.TK.dims, prism_drb_ltm.TK.values,\n",
    "                         prism_drb_ltm.TK.attrs, prism_drb_ltm.TK.encoding)\n",
    "\n",
    "prism_out_grid[\"PREC_ACC_NC\"] = (prism_drb_ltm.PREC_ACC_NC.dims, prism_drb_ltm.PREC_ACC_NC.values,\n",
    "                         prism_drb_ltm.PREC_ACC_NC.attrs, prism_drb_ltm.PREC_ACC_NC.encoding)\n",
    "\n",
    "# groupby\n",
    "# prism_grouped = prism_out_grid.drop_vars(\"spatial_ref\").groupby(prism_out_grid.huc6)\n",
    "\n",
    "# CERES-EBAF\n",
    "\n",
    "# create an output grid\n",
    "ceres_out_grid = make_geocube(\n",
    "    vector_data = drb_gdf,\n",
    "    measurements=[\"huc6\"],\n",
    "    like=ceres_drb_ltm.rio.write_crs(c404_crs)\n",
    ")\n",
    "\n",
    "# add datarrays to grid\n",
    "ceres_out_grid[\"RNET\"] = (ceres_drb_ltm.RNET.dims, ceres_drb_ltm.RNET.values,\n",
    "                         ceres_drb_ltm.RNET.attrs, ceres_drb_ltm.RNET.encoding)\n",
    "\n",
    "# groupby\n",
    "# ceres_grouped = ceres_out_grid.drop_vars(\"spatial_ref\").groupby(ceres_out_grid.huc6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prism_PREC_ACC_NC_bias = c404_out_grid.PREC_ACC_NC - prism_out_grid.PREC_ACC_NC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prism_PREC_ACC_NC_bias[\"huc6\"] = c404_out_grid.huc6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prism_PREC_ACC_NC_bias.groupby(\"huc6\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Areal statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../Metrics_NWM_StdSuite_v1.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(obs, sim):\n",
    "    return np.mean(np.abs(obs - sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"prism:\", prism_drb_ltm.data_vars, \"\\nceres\", ceres_drb_ltm.data_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "areal_stats = {\n",
    "    \"c404_PREC_ACC_NC_mean\" : c404_drb_ltm.PREC_ACC_NC.compute().mean().data.tolist(),\n",
    "    \"prism_PREC_ACC_NC_mean\" : prism_drb_ltm.PREC_ACC_NC.compute().mean().data.tolist(),\n",
    "    \"c404_PREC_ACC_NC_med\" : c404_drb_ltm.PREC_ACC_NC.compute().median().data.tolist(),\n",
    "    \"prism_PREC_ACC_NC_med\" : prism_drb_ltm.PREC_ACC_NC.compute().median().data.tolist(),\n",
    "    \"c404_PREC_ACC_NC_std\" : c404_drb_ltm.PREC_ACC_NC.compute().std().data.tolist(),\n",
    "    \"prism_PREC_ACC_NC_std\" : prism_drb_ltm.PREC_ACC_NC.compute().std().data.tolist(),\n",
    "    \"c404_TK_mean\" : c404_drb_ltm.TK.compute().mean().data.tolist(),\n",
    "    \"prism_TK_mean\" : prism_drb_ltm.TK.compute().mean().data.tolist(),\n",
    "    \"c404_TK_med\" : c404_drb_ltm.TK.compute().median().data.tolist(),\n",
    "    \"prism_TK_med\" : prism_drb_ltm.TK.compute().median().data.tolist(),\n",
    "    \"c404_TK_std\" : c404_drb_ltm.TK.compute().std().data.tolist(),\n",
    "    \"prism_TK_std\" : prism_drb_ltm.TK.compute().std().data.tolist(),\n",
    "    \"c404_RNET_mean\" : c404_drb_ltm.RNET.compute().mean().data.tolist(),\n",
    "    \"ceres_RNET_mean\" : ceres_drb_ltm.RNET.compute().mean().data.tolist(),\n",
    "    \"c404_RNET_med\" : c404_drb_ltm.RNET.compute().median().data.tolist(),\n",
    "    \"ceres_RNET_med\" : ceres_drb_ltm.RNET.compute().median().data.tolist(),\n",
    "    \"c404_RNET_std\" : c404_drb_ltm.RNET.compute().std().data.tolist(),\n",
    "    \"ceres_RNET_std\" : ceres_drb_ltm.RNET.compute().std().data.tolist(),\n",
    "    \"prism_PREC_ACC_NC_corr\" : xr.corr(c404_drb_ltm.PREC_ACC_NC, prism_drb_ltm.PREC_ACC_NC).compute().data.tolist(),\n",
    "    \"prism_TK_corr\" : xr.corr(c404_drb_ltm.TK, prism_drb_ltm.TK).compute().data.tolist(),\n",
    "    \"ceres_RNET_corr\" : xr.corr(c404_drb_ltm.RNET, ceres_drb_ltm.RNET).compute().data.tolist(),\n",
    "    \"prism_PREC_ACC_NC_mae\" : mae(c404_drb_ltm.PREC_ACC_NC, prism_drb_ltm.PREC_ACC_NC).compute().data.tolist(),\n",
    "    \"prism_TK_mae\" : mae(c404_drb_ltm.TK, prism_drb_ltm.TK).compute().data.tolist(),\n",
    "    \"ceres_RNET_mae\" : mae(c404_drb_ltm.RNET, ceres_drb_ltm.RNET).compute().data.tolist(),\n",
    "    \"prism_PREC_ACC_NC_rmsd\" : np.sqrt(MSE(c404_drb_ltm.PREC_ACC_NC, prism_drb_ltm.PREC_ACC_NC)).compute().data.tolist(),\n",
    "    \"prism_TK_rmsd\" : np.sqrt(MSE(c404_drb_ltm.TK, prism_drb_ltm.TK)).compute().data.tolist(),\n",
    "    \"ceres_RNET_rmsd\" : np.sqrt(MSE(c404_drb_ltm.RNET, ceres_drb_ltm.RNET)).compute().data.tolist()\n",
    "}\n",
    "\n",
    "areal_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_drb_ltm.PREC_ACC_NC.median().data.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c404_drb_ltm.PREC_ACC_NC.min(), c404_drb_ltm.PREC_ACC_NC.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c404_drb_ltm.PREC_ACC_NC.std().data.tolist(), prism_drb_ltm.PREC_ACC_NC.std().data.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Extract gridded values to points**\n",
    "\n",
    "The goal of this section is extract values from CONUS404 where they intersect with station data. This process is described in article about the ESRI tool [Extract Values to Points](https://pro.arcgis.com/en/pro-app/latest/tool-reference/spatial-analyst/extract-values-to-points.htm). This tabular data will then be exported for use in the next notebook, **CONUS404 Analysis**.\n",
    "\n",
    "Dataset outline:\n",
    "1. Read in the prepared dataset\n",
    "2. Extract data from overlapping pixel at same time step as point\n",
    "<br>\n",
    "\n",
    "**Climate Reference Network point extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = fsspec.filesystem(\"s3\", anon=False, requester_pays=True, skip_instance_cache=True)\n",
    "\n",
    "crn_drb_df = pd.read_parquet(fs.open(\"s3://nhgf-development/workspace/tutorial/CONUS404/crn_drb.parquet\"))\n",
    "\n",
    "# create geodataframe\n",
    "crn_drb = gpd.GeoDataFrame(crn_drb_df, crs=4326,\n",
    "                       geometry=gpd.points_from_xy(crn_drb_df.LONGITUDE, \n",
    "                                                         crn_drb_df.LATITUDE))\n",
    "\n",
    "# modify date field\n",
    "crn_drb[\"DATE\"] = crn_drb[\"DATE\"].astype(str).str[:-3]\n",
    "\n",
    "crn_drb.rename({\"DATE\": \"time\",\n",
    "                \"TK\": \"crn_TK\", \n",
    "                \"RNET\": \"crn_RNET\", \n",
    "                \"PREC_ACC_NC\": \"crn_PREC_ACC_NC\"},\n",
    "                  axis=1, inplace=True)\n",
    "\n",
    "crn_drb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get coordinates from crn_drb to index c404_drb by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# isolate single row and transform to c404_drb crs\n",
    "crn_coords_gdf = crn_drb.iloc[[0]].to_crs(c404_crs)\n",
    "\n",
    "# extract lat/long values\n",
    "crn_lat = crn_coords_gdf.iloc[0][\"geometry\"].y\n",
    "crn_lon = crn_coords_gdf.iloc[0][\"geometry\"].x\n",
    "\n",
    "# time\n",
    "crn_time_min = crn_drb_df[\"time\"].min()\n",
    "crn_time_max = crn_drb_df[\"time\"].max()\n",
    "crn_time_min, crn_time_max\n",
    "\n",
    "# subset c404_drb to lat/long using nearest\n",
    "c404_crn_sub = c404_drb.sel(x=crn_lon, y=crn_lat, method=\"nearest\")\n",
    "\n",
    "# slice to time-steps of crn_drb\n",
    "c404_crn_sub = c404_crn_sub.sel(time=slice(crn_time_min, crn_time_max))\n",
    "\n",
    "c404_crn_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert subset to dataframe and reorganize columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c404_sub_crn_df = c404_crn_sub.to_dataframe().reset_index(drop=False)\n",
    "\n",
    "# trim columns\n",
    "c404_sub_crn_df = c404_sub_crn_df[[\"time\", \"TK\", \"RNET\", \"PREC_ACC_NC\"]]\n",
    "\n",
    "# rename columns\n",
    "c404_sub_crn_df.rename({\"TK\": \"c404_TK\", \n",
    "                    \"RNET\": \"c404_RNET\", \n",
    "                    \"PREC_ACC_NC\": \"c404_PREC_ACC_NC\"},\n",
    "                  axis=1, inplace=True)\n",
    "\n",
    "# trim time\n",
    "c404_sub_crn_df[\"time\"] = c404_sub_crn_df[\"time\"].astype(str).str[:-3]\n",
    "\n",
    "c404_sub_crn_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine CONUS404 subset with CRN data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crn_c404_point = crn_drb.merge(c404_sub_crn_df, on=\"time\").reset_index(drop=False)\n",
    "\n",
    "# drop columns\n",
    "crn_c404_point.drop([\"index\", \"LATITUDE\", \"LONGITUDE\", \"ID\", \"geometry\"], axis=1, inplace=True)\n",
    "\n",
    "crn_c404_point.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crn_c404_point.to_parquet(\"s3://nhgf-development/workspace/tutorial/CONUS404/crn_c404_point.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Historical Climate Network (HCN) point extraction**\n",
    "\n",
    "The HCN data is different than the CRN data as the HCN data comes from multiple stations whereas the CRN data was from a single station. This will involve using multiple sets of geographic coordinates to extract data from CONUS404."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the HCN dataset\n",
    "fs = fsspec.filesystem(\"s3\", anon=False, requester_pays=True, skip_instance_cache=True)\n",
    "\n",
    "hcn_drb_df = pd.read_parquet(fs.open(\"s3://nhgf-development/workspace/tutorial/CONUS404/hcn_drb.parquet\"))\n",
    "\n",
    "#rename columns\n",
    "hcn_drb_df.rename({\"DATE\": \"time\",\n",
    "                \"TK\": \"hcn_TK\",  \n",
    "                \"PREC_ACC_NC\": \"hcn_PREC_ACC_NC\"},\n",
    "                  axis=1, inplace=True)\n",
    "\n",
    "# change DATE field to \n",
    "hcn_drb_df[\"time\"] = hcn_drb_df[\"time\"].astype(str).str[:-3]\n",
    "\n",
    "hcn_drb_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a DataFrame of the station IDs, lats, and longs to use for extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hcn_stations = hcn_drb_df.copy().drop([\"time\", \"hcn_TK\", \"hcn_PREC_ACC_NC\"], axis=1)\n",
    "hcn_stations[\"LONGITUDE\"] = pd.to_numeric(hcn_stations[\"LONGITUDE\"])\n",
    "hcn_stations[\"LATITUDE\"] = pd.to_numeric(hcn_stations[\"LATITUDE\"])\n",
    "\n",
    "hcn_stations = hcn_stations.groupby('ID').mean().reset_index(drop=False)\n",
    "# hcn_stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a GeoDataFrame to convert the lat and long to the coordinate system of CONUS404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcn_stations_gdf = gpd.GeoDataFrame(hcn_stations, crs=4326,\n",
    "                       geometry=gpd.points_from_xy(hcn_stations.LONGITUDE, \n",
    "                                                         hcn_stations.LATITUDE))\n",
    "\n",
    "# transform to c404_drb crs\n",
    "hcn_stations_gdf = hcn_stations_gdf.to_crs(c404_crs)\n",
    "\n",
    "# extract lat/long values\n",
    "hcn_stations_gdf[\"y\"] = hcn_stations_gdf[\"geometry\"].y\n",
    "hcn_stations_gdf[\"x\"] = hcn_stations_gdf[\"geometry\"].x\n",
    "\n",
    "#drop lat/lon/geo\n",
    "hcn_stations_df = hcn_stations_gdf.drop([\"LATITUDE\", \"LONGITUDE\", \"geometry\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset c404_drb to time period of HCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time min/max\n",
    "hcn_time_min = hcn_drb_df[\"time\"].min()\n",
    "hcn_time_max = hcn_drb_df[\"time\"].max()\n",
    "\n",
    "# slice c404 to HCN time\n",
    "c404_hcn_timesub = c404_drb.sel(time=slice(hcn_time_min, hcn_time_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Dataframe rows to extract data from c404_drb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# list of extracted data\n",
    "c404_hcn_subs = []\n",
    "\n",
    "for index, data in hcn_stations_df.iterrows():\n",
    "    c404_hcn_sub_step = c404_hcn_timesub.sel(x=data.x, y=data.y, method=\"nearest\").to_dataframe()\n",
    "    c404_hcn_sub_step[\"ID\"] = data.ID\n",
    "    c404_hcn_subs.append(c404_hcn_sub_step)\n",
    "\n",
    "# concat list of extracted data into single Dataframe\n",
    "c404_hcn_sub = pd.concat(c404_hcn_subs)\n",
    "\n",
    "#reset index\n",
    "c404_hcn_sub.reset_index(drop=False, inplace=True)\n",
    "\n",
    "# drop columns\n",
    "c404_hcn_sub.drop([\"RNET\", \"lon\", \"lat\", \"y\", \"x\", \"crs\"], axis=1, inplace=True)\n",
    "\n",
    "# rename columns\n",
    "c404_hcn_sub.rename({\"TK\":\"c404_TK\",\n",
    "                    \"PREC_ACC_NC\": \"c404_PREC_ACC_NC\"},\n",
    "                   axis=1, inplace=True)\n",
    "\n",
    "# trim time\n",
    "c404_hcn_sub[\"time\"] = c404_hcn_sub[\"time\"].astype(str).str[:-3]\n",
    "\n",
    "# c404_hcn_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge CONUS404 observations to HCH observations using the station ID and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcn_c404_point = hcn_drb_df.merge(c404_hcn_sub, left_on=[\"ID\", \"time\"], right_on=[\"ID\", \"time\"])\n",
    "\n",
    "# drop columns\n",
    "hcn_c404_point.drop([\"LATITUDE\", \"LONGITUDE\"], axis=1, inplace=True)\n",
    "\n",
    "hcn_c404_point.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcn_c404_point.to_parquet(\"s3://nhgf-development/workspace/tutorial/CONUS404/hcn_c404_point.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check all of the files that have been created in the data preparation notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = fsspec.filesystem(\"s3\", anon=False, requester_pays=True, skip_instance_cache=True)\n",
    "\n",
    "fs.ls(\"s3://nhgf-development/workspace/tutorial/CONUS404\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shut down the client and cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close(); cluster.shutdown()\n",
    "del client; del cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Next: CONUS404 Analysis notebook\n",
    "\n",
    "Now that we have moved through our data preparation and calculated zonal and point statistics, we can move on to analyzing the differences between CONUS404 and the reference data in the CONUS404 Analysis notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Last code cell of the notebook\n",
    "# import watermark.watermark as watermark\n",
    "# print(watermark(iversions=True, python=True, machine=True, globals_=globals()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "users-pangeo",
   "language": "python",
   "name": "conda-env-users-pangeo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "6b8b815d206080047d0881750c260f2e84eb4576ca4137e2355fa3de469693c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
