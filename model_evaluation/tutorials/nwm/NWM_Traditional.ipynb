{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# National Water Model Benchmarking Workflow (traditional metrics)\n",
    "\n",
    "**NOTE**: \n",
    "_This notebook adapted from originals by Timothy Hodson and Rich Signell. See that upstream work at:_\n",
    "* https://github.com/thodson-usgs/dscore\n",
    "* https://github.com/USGS-python/hytest-evaluation-workflows/\n",
    "\n",
    "\n",
    "---\n",
    "See the [overview](./NWM_traditional.ipynb) in this folder for a summary of the benchmarking notebooks. \n",
    "\n",
    "\n",
    "<details>\n",
    "  <summary>Guide to pre-requisites and learning outcomes...&lt;click to expand&gt;</summary>\n",
    "  \n",
    "  <table>\n",
    "    <tr>\n",
    "      <td>Pre-Requisites\n",
    "      <td>To get the most out of this notebook, you should already have an understanding of these topics: \n",
    "        <ul>\n",
    "        <li>pre-req one\n",
    "        <li>pre-req two\n",
    "        </ul>\n",
    "    <tr>\n",
    "      <td>Expected Results\n",
    "      <td>At the end of this notebook, you should be able to: \n",
    "        <ul>\n",
    "        <li>outcome one\n",
    "        <li>outcome two\n",
    "        </ul>\n",
    "  </table>\n",
    "</details>\n",
    "\n",
    "## Essential Benchmark Components\n",
    "This benchmark notebook will present a workflow which follows a canonical model for Essential Benchmark Components: \n",
    "1) A set of predictions and matching observations; \n",
    "2) The domain (e.g. space or time) over which to benchmark;\n",
    "3) A set of statistical metrics with which to benchmark. \n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Load libraries and configure Python computing environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get access to helper library\n",
    "%run ../../setup.ipynb\n",
    "\n",
    "# Python libraries we will need...\n",
    "import pandas as pd\n",
    "import logging\n",
    "from HyTEST.benchmarks.NWMStandardSuite import NWMStandardSuite\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will this ananlysis need parallelism?  Likely yes.  \n",
    "\n",
    "The following cell will configure a cluster environment suited to the server hosting this notebook. \n",
    "\n",
    "You may let our `configure_cluster()` helper try to guess the cluster config for you, or you can \n",
    "explicitly name a config matching where you are running this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HyTEST.helpers import configure_cluster\n",
    "(client, cluster) = configure_cluster('local')\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Data\n",
    "\n",
    "Essential Benchmark Components: \n",
    "1) A set of predictions and matching observations,  <span style=\"color:red; font-size:large\"><<--You are here</span>\n",
    "2) The domain over which to benchmark \n",
    "3) A set of statistical metrics with which to benchmark. \n",
    "\n",
    "Finding and loading data is made easier for this particular workflow, in tht most of it has been pre-processed and stored in a cloud-friendly format. That data store is indexed by an **intake catalog**.  Learn more about `intake` [here](../L2/xx_Intake.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import intake\n",
    "try:\n",
    "    # __hytest_intake_catalog_URL__ should have been defined within 'setup'... or you can override here:\n",
    "    url = __hytest_intake_catalog_URL__ \n",
    "    # to over-ride, edit and un-comment the following:\n",
    "    # url = r\"http://path/to/your/custom/catalog.yml\"\n",
    "except NameError:\n",
    "    # A default fall-back catalog\n",
    "    url = r'https://raw.githubusercontent.com/nhm-usgs/data-pipeline-helpers/main/hytest/hytest_intake_catalog.yml'\n",
    "\n",
    "data_catalog = intake.open_catalog(url); del url\n",
    "print(\"Available datasets: \\n\", \"\\n\".join(data_catalog.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above list represents the processed datasets available for benchmarking.  If a dataset\n",
    "you want is not in that list, you can contact Rich Signell to get it added, or load the \n",
    "data manually using [other means](../../L2/LoadingData.ipynb). If you load data from a \n",
    "source other than this list, you can jump to [Step 2: Restrict to a Domain](#step-2-restrict-to-a-domain)\n",
    "\n",
    "Note that the interesting datasets in the cataloged dataset above are duplicated: Some are `-onprem` \n",
    "and some are `-cloud`. Same data, but the storage location and access protocol will be different. You \n",
    "will definitely want to use the correct copy of the data for your computing environment.  \n",
    "* `onprem` : Direct access via the `caldera` filesystem from _denali_ or _tallgrass_\n",
    "* `cloud` : Network access via S3 bucket, suitable for consumption on cloud-hosted jupyter servers. You could also access using any network-attached computer, but the amount of data will likely saturate your connection.  Use in the cloud (e.g. ESIP QHub)\n",
    "\n",
    "So... are you on-prem? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HyTEST.helpers import onprem\n",
    "if onprem():\n",
    "    print(\"Yes : -onprem\")  #<-- likely you are running on tallgrass or denali\n",
    "    obs_data_src='nwis-streamflow-usgs-gages-onprem'\n",
    "    mod_data_src='nwm21-streamflow-usgs-gages-onprem'\n",
    "else:\n",
    "    print(\"Not onprem; use '-cloud' data source\")\n",
    "    obs_data_src='nwis-streamflow-usgs-gages-cloud'\n",
    "    mod_data_src='nwm21-streamflow-usgs-gages-cloud'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have that sorted... let's get data into memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_of_interest = 'streamflow'\n",
    "try:\n",
    "    obs = data_catalog[obs_data_src].to_dask()\n",
    "    mod = data_catalog[mod_data_src].to_dask()\n",
    "except KeyError:\n",
    "    print(\"Something wrong with dataset names.\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    obs_data = obs[variable_of_interest]\n",
    "    mod_data = mod[variable_of_interest].astype('float32')\n",
    "except KeyError:\n",
    "    print(f\"{variable_of_interest} was not found in these data.\")\n",
    "\n",
    "obs_data.name = 'observed'\n",
    "mod_data.name = 'predicted'    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Restrict to a Domain\n",
    "\n",
    "Essential Benchmark Components: \n",
    "1) A set of predictions and matching observations,  \n",
    "2) The domain over which to benchmark <span style=\"color:red; font-size:large\"><<--You are here</span>\n",
    "3) A set of statistical metrics with which to benchmark. \n",
    "\n",
    "Each benchmark domain is defined over specific bounds (typically space and/or time). \n",
    "Benchmark domain definitions are published to Science Base, or they can be defined within the notebook. \n",
    "\n",
    "This notebook will use a benchmark domain definition loaded from ESIP's network file \n",
    "system (S3). It is essentially a list of stream guages in which we are interested, along with some \n",
    "metadata about that gage (watershed, reach code, etc).  We will use this as a spatial selector\n",
    "to restrict the data to only those gages found within this benchmarking domain.\n",
    "\n",
    "Because we are using a standardized list, we need to fetch it from its upstream source, which is\n",
    "an S3 bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fsspec\n",
    "fs = fsspec.filesystem('s3', anon=True)\n",
    "url = 's3://esip-qhub-public/usgs/hytest/streamflow_benchmark_sites_v09.csv'\n",
    "try:\n",
    "    domain_data = pd.read_csv(\n",
    "        fs.open(url), \n",
    "        dtype={'site_no':str, 'huc_cd':str, 'reachcode':str, 'comid':str },\n",
    "        index_col='site_no'\n",
    "        )\n",
    "except:\n",
    "    print(f\"Could not open the file at {url}... AWS problem?\")\n",
    "    raise\n",
    "# Re-format the gage_id/site_no string value.  ex:   \"1000000\"  ==> \"USGS-1000000\"\n",
    "domain_data.rename(index=lambda x: f'USGS-{x}', inplace=True)\n",
    "print(f\"{len(domain_data.index)} gages in this benchmark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have a domain dataset representing the stream gages (unique `site_no` values) identifying the locations making up the spatial domain of this benchmark. While we have good meta-data for these gages (lat/lon location, HUC8 code, etc), we really will only use the list of `site_no` values to pull values out of the raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Compute Metrics\n",
    "\n",
    "Essential Benchmark Components: \n",
    "1) A set of predictions and matching observations,  \n",
    "2) The domain over which to benchmark \n",
    "3) A set of statistical metrics with which to benchmark. <span style=\"color:red; font-size:large\"><<--You are here</span>\n",
    "\n",
    "The code to calculate the various metrics has been standardized and moved into a 'helper' library. You can use these or write your\n",
    "own.  Either way, we need to put all metric computation into a special all-encompasing benchmarking function--a single call which can be assigned to each gage in our domain list. This sort of arrangement is well-suited to parallelism with `dask`. If this is done well, the process will benefit enormously from task parallelism -- each gage can be given its own CPU to run on.  After all are done, the various results will be collected and assembled into a composite dataset. \n",
    "\n",
    "To achieve this, we need a single 'atomic' function that can execute independently. It will take the gage identifier as input and return a list of metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Wrapper function -- this func will be called once per gage_id, each call on its own dask worker\n",
    "def compute_benchmark(gage_id):\n",
    "    try:\n",
    "        ## obs_data and mod_data should be globals...\n",
    "        obs = obs_data.sel(gage_id=gage_id).load(scheduler='single-threaded').to_series()\n",
    "        mod = mod_data.sel(gage_id=gage_id).load(scheduler='single-threaded').to_series().resample('1D', offset='5h').mean() \n",
    "        \n",
    "        # make sure the indices match\n",
    "        obs.index = obs.index.to_period('D')\n",
    "        mod.index = mod.index.to_period('D')\n",
    "\n",
    "        # merge obs and predictions and drop NaNs.\n",
    "        gage_df = pd.merge(obs, mod, left_index=True, right_index=True).dropna(how='any')\n",
    "        \n",
    "        nwm = NWMStandardSuite.from_df(gage_df, 'observations', 'predictions')\n",
    "        #     ^^^^^^^^^^^^^^^^ This is the 'helper' to calc all of the metrics we want.\n",
    "        scores = nwm.suite()\n",
    "        scores.name = gage_id\n",
    "        return scores\n",
    "    except: #<-- this is an extremely broad way to catch exceptions.  We only do it this way to ensure \n",
    "            #    that a failure on one benchmark (for a single stream gage) will not halt the entire run. \n",
    "        logging.info(\"Benchmark failed for %s\", gage_id)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test to be sure this `compute_benchmark()` function will return data for a single gage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_benchmark('USGS-01030350')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to set up a way to farm out this function, once per gage ID. `dask` will do this.  \n",
    "\n",
    "For each gage ID in a list (the list we got in Step 2, above), `dask`` will call this function with that gage \n",
    "ID as a parameter.  We'll use a dask `bag` (a type of parallelizable collection), and use it to \n",
    "`map()` a function to the bag contents contents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.bag as db\n",
    "\n",
    "bag = db.from_sequence(domain_data['site_no'].tolist()).map(compute_benchmark)\n",
    "#     ^^^^^^^^^^^^^^^^      ^^^^^^^^^^                  ^^^          ^^^--The function to connect/map to each list item.\n",
    "#     Creates the bag     with this list as contents    map <==> one-to-one connection\n",
    "\n",
    "results = bag.compute() # dask will not actually compute results until you ask. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that big task done, we don't need `dask` parallelism any more. Let's shut down the cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close(); del client\n",
    "cluster.close(); del cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assemble the results\n",
    "The `bag` now contains a collection of return values (one per call to `compute_benchmark()`).  We can massage that into a table/dataframe for easier processing: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [i for i in results if i is not None] # Drop entries where compute_benchmark failed\n",
    "\n",
    "results_df = pd.concat(results, axis=1).T\n",
    "results_df.index.name = 'site_no'\n",
    "#ds_results = df_results.to_xarray()\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataframe can be saved to disk -- it will be used for visualizations in other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('NWM_v2.1_streamflow_benchmark.csv')\n",
    "#ds_results.merge(benchmark_ds, join='inner').to_netcdf('nwm_v2.1_streamflow_benchmark.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fingerprint())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('hytest')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "4100cc85ffefb381c538d28dd18cb927e5a99f05bbed6aaad5313d7bb1c2079e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
