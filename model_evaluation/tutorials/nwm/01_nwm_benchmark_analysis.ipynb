{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e11273a-f755-46a7-994d-7bb4e5367e9e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#\n",
    "# HyTEST hydrologic model benchmark assessment: standard metric analyses\n",
    "\n",
    "**Timothy Hodson and Rich Signell**\n",
    "\n",
    "## About\n",
    "This notebook demonstrates a computational workflow for benchmarking daily streamflow simulated from the National Water Model Retrospective version 2.1, and is meant to provide an adaptable template for benchmarking other Earth-system models and datasets. \n",
    "\n",
    "The HyTEST benchmark workflow consists of three components:\n",
    "1. a set of model predictions and observations, or evaluation data, to compare.\n",
    "1. the spatiotemporal domain over which to compute benchmark results.\n",
    "1. a set of statistical metrics with which to generate benchmark results. In this notebook, we focus on the standard statistical metric suite version 1.0, sometimes referred to as the traditional statistical metrics.\n",
    "\n",
    "The workflow loads the model predictions and observations, subsets the data to the specified domain of the benchmark, and finally calculates metrics for the given data over that given domain. Any benchmark result is fully reproducible, given a workflow notebook and the correct versions of each of these three components.\n",
    "\n",
    "In practice, the datasets may be too large to fit in memory or to transfer, so this notebook will demonstrate several open-source Python libraries for 'moving the computations to the data.' Some of these tools are relatively new, but they are quickly becoming standards within the Earth-science community.\n",
    "\n",
    "The notebook is organized into a series of helper functions that handle tasks like loading data, configuring compute resources, and computing metrics over a chunk of data. Once these are defined, the analysis can be run in a few lines of code. The output generated from this notebook can serve as the beginning step in a workflow notebook specified to visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32021b3-9141-4acb-9256-b66cf9a08c0d",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "### 0.0. Load libraries\n",
    "Prior to beginning, ensure that following Python librariers are installed and loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa7e186-83be-4dae-87d8-d469beb4a147",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_jobqueue import SLURMCluster\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import dask.bag as db\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import intake\n",
    "import dask\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1db8121-7e23-4b52-8694-4e9fa1a17478",
   "metadata": {},
   "source": [
    "### 0.1. Configure cluster\n",
    "The notebook shows example configurations that might be used for three different computing resources supported by USGS, including Denali, Tallgrass, and Cloud.\n",
    "\n",
    "First, select the computing resource on which to run your analysis. Options include:\n",
    "1) denali\n",
    "2) tallgrass\n",
    "3) local\n",
    "4) esip-qhub-gateway-v0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bdcad1-d21d-4340-99ac-9d4ad032685c",
   "metadata": {},
   "outputs": [],
   "source": [
    "resource = 'tallgrass' #denali, tallgrass, local, esip-qhub-gateway-v0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bdb0cd-89ec-4f07-bf68-b60b8e231140",
   "metadata": {},
   "source": [
    "How to configure the cluster will vary among these resources, so we've created a helper function to take care of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9abd71d-312d-4c5f-b520-077c80862314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_cluster(resource):\n",
    "    ''' Helper function to configure cluster\n",
    "    '''\n",
    "    if resource == 'denali':\n",
    "        cluster = LocalCluster(threads_per_worker=1)\n",
    "        client = Client(cluster)\n",
    "    \n",
    "    elif resource == 'tallgrass':\n",
    "        project = os.environ['SLURM_JOB_ACCOUNT']\n",
    "        \n",
    "        cluster = SLURMCluster(processes=1,cores=1, \n",
    "            memory='10GB', interface='ib0',\n",
    "            project=project, walltime='01:00:00',      \n",
    "            job_extra={'hint': 'multithread'})\n",
    "        cluster.scale(10)\n",
    "        client = Client(cluster)\n",
    "        \n",
    "    elif resource == 'local':\n",
    "        import warnings\n",
    "        warnings.warn(\"Running locally can result in costly data transfers!\\n\")\n",
    "        n_cores = os.cpu_count() # set to match your machine\n",
    "        cluster = LocalCluster(threads_per_worker=n_cores)\n",
    "        client = Client(cluster)\n",
    "        \n",
    "    elif resource in ['esip-qhub-gateway-v0.4']:   \n",
    "        import sys\n",
    "        sys.path.append(os.path.join(os.environ['HOME'],'shared','users','lib'))\n",
    "        import ebdpy as ebd\n",
    "        ebd.set_credentials(profile='esip-qhub')\n",
    "\n",
    "        aws_profile = 'esip-qhub'\n",
    "        aws_region = 'us-west-2'\n",
    "        endpoint = f's3.{aws_region}.amazonaws.com'\n",
    "        ebd.set_credentials(profile=aws_profile, region=aws_region, endpoint=endpoint)\n",
    "        worker_max = 30\n",
    "        client,cluster = ebd.start_dask_cluster(profile=aws_profile, worker_max=worker_max, \n",
    "                                              region=aws_region, use_existing_cluster=True,\n",
    "                                              adaptive_scaling=False, wait_for_cluster=False, \n",
    "                                              worker_profile='Medium Worker', propagate_env=True)\n",
    "        \n",
    "    return client, cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8713623c-d507-4213-b534-5204bd9e56a9",
   "metadata": {},
   "source": [
    "## 1. Define performance benchmark\n",
    "A performance benchmark consists of three components: (1) a set of predictions and observations, (2) the domain over which to benchmark (3) a set of statistical metrics with which to produce benchmark results. The basic workflow is to load the predictions and observations, subset them to the domain of the benchmark, then calculate metrics on the data over that domain.\n",
    "\n",
    "### 1.0 Load data\n",
    "Let's begin by introducing [Intake](https://github.com/intake/intake), which is a set of tools for loading and sharing data in data science projects. Data from this project are stored within an Intake catalog. We can inpsect that catalog with the following lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7743279-a185-408f-81cf-0aedad65c085",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/USGS-python/hytest-catalogs/main/hytest_intake_catalog.yml'\n",
    "cat = intake.open_catalog(url)\n",
    "print(list(cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956fd92a-1659-4192-b7b5-367a823f266e",
   "metadata": {},
   "source": [
    "Above, you should see several or more data filenames of models, data hubs, and location descriptions.\n",
    "\n",
    "- Files ending in \"-onprem\" are data that are located on HPC resources. Files ending in \"-cloud\", pertain to data housed on cloud, and \"-esip\" pertain to data housed in qhub. \n",
    "- Files with 'conus404' in the description pertain to the CONUS404 dataset. Uncalibrated, calibrated? DOI to cite?\n",
    " - Files with 'nwm21' pertain to the National Water Model retrospective version 2.1, and may have a variable such as 'streamflow' to designate the simulated variable.\n",
    "- Files with 'nwis' pertain to data from USGS National Water Inventory System (NWIS) which is observational streamflow data, parameter code '00060', stat_cd = \"00003\" (mean)\n",
    "- Files with 'streamflow' pertain to the variable that is housed in that data file.\n",
    "\n",
    "Using the Intake catalog, we define another helper function that loads our data from the appropriate location depending on where the computation will be run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32aaac3f-dd7b-4d8e-9a02-2f97efe00d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations_ds = cat[f'nwis-streamflow-usgs-gages-onprem'].to_dask()\n",
    "observations_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258ddbbe-ebd2-452c-ac86-5fd20831181d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_streamflow_data(resource):\n",
    "    ''' Helper function to load observations and model predictions from Intake.\n",
    "    \n",
    "    Some initial preprocessing is also done here, like converting the datasets to the same type.\n",
    "    '''\n",
    "    if resource in ['tallgrass','denali']:\n",
    "        location = 'onprem'\n",
    "        \n",
    "    elif resource in ['esip-qhub-gateway-v0.4']:\n",
    "        location = 'cloud'\n",
    "\n",
    "    url = 'https://raw.githubusercontent.com/USGS-python/hytest-catalogs/main/hytest_intake_catalog.yml'\n",
    "    cat = intake.open_catalog(url)\n",
    "\n",
    "    observations_ds = cat[f'nwis-streamflow-usgs-gages-{location}'].to_dask()\n",
    "    model_ds = cat[f'nwm21-streamflow-usgs-gages-{location}'].to_dask()\n",
    "    \n",
    "    observations = observations_ds['streamflow']\n",
    "    model = model_ds['streamflow'].astype('float32')\n",
    "\n",
    "    observations.name = 'observed'\n",
    "    model.name = 'predicted'\n",
    "    \n",
    "    return observations, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95c773a-9ec3-42b5-8d1b-66d0b9735181",
   "metadata": {},
   "source": [
    "Let's demo that helper, and show how to select data for a single streamgage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bed5fb-6f3f-408f-be98-abc0f29665cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, pred = load_streamflow_data(resource)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663ce1de-7b05-4afb-87a1-334ef2b22689",
   "metadata": {},
   "outputs": [],
   "source": [
    "#obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c7625e-d67c-41bc-9c2e-271542546135",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9550137d-e140-4ce7-b101-9eaeaa7cc13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# time it takes to read a single gage\n",
    "gage_id = 'USGS-01030350'\n",
    "obs.sel(gage_id=gage_id).load(scheduler='threads').to_series().tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b8921f-5b2c-43fc-a3d7-98ba79972421",
   "metadata": {},
   "source": [
    "### 1.1 Load benchmark locations\n",
    "Each benchmark is defined over a specific domain (typically bounded in space and time, with the two related in some aspects). Benchmark spatiotemporal domains are published to ScienceBase within the [HyTEST directory](https://www.sciencebase.gov/catalog/item/61dd751ed34ed7929401a4bd), or they can be defined within the notebook if a user chooses. For this example, we use the Cobalt gages, avaliable for download on ScienceBase ([Foks et al., 2022](https://doi.org/10.5066/P972P42Z))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b33e0ed-cbd2-4fa1-8213-c27314de6c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fsspec\n",
    "#fs = fsspec.filesystem('https', anon=True)\n",
    "url = 'https://raw.githubusercontent.com/USGS-python/hytest-evaluation-workflows/main/misc/streamflow_gages_v1_n5390.csv'\n",
    "benchmark_ds = pd.read_csv(url, dtype={'site_no':str, 'huc_cd':str, 'reachcode':str, 'comid':str, 'gagesII_class':str, 'aggecoregion': str}).set_index('site_no').to_xarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9370c57-51de-4361-855b-874dfc7c2838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the site_no\n",
    "benchmark_ds['site_no'] = [f'USGS-{site}' for site in benchmark_ds['site_no'].values]\n",
    "benchmark_gages = benchmark_ds['site_no'].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c78ae4-9da7-4461-aa30-2a2c68e67dc7",
   "metadata": {},
   "source": [
    "### 1.2. Load statistical metrics\n",
    "This demo computes a benchmark for the National Water Model (NWM) Retrospective version 2.1 using a suite of traditional metrics (referred to as the \"standard statistical suite version 1.0\"). In practice these may be loaded from external libraries. Ideally, the future version of this notebook would be configured to load a specific version of that library, which would be used to uniquely identify the benchmark to run. Additionally, future specific metrics packages could be loaded with `httpimport`. \n",
    "\n",
    "Below is the definition of the version 1.0 standard statistical suite (10 metrics total; Towler et al., in draft) that is calculated in this notebook, with references.\n",
    "\n",
    "| Metric | Reference |\n",
    "| :----------- | :----------- |\n",
    "| Nash-Sutcliffe efficiency (NSE) | Nash, J. E., & Sutcliffe, J. V. (1970). River flow forecasting through conceptual models part I—A discussion of principles. Journal of hydrology, 10(3), 282-290. |\n",
    "| Kling-Gupta efficiency (KGE) | Gupta, H. V., Kling, H., Yilmaz, K. K., & Martinez, G. F. (2009). Decomposition of the mean squared error and NSE performance criteria: Implications for improving hydrological modelling. Journal of hydrology, 377(1-2), 80-91. |\n",
    "| logNSE | Oudin, L., Andréassian, V., Mathevet, T., Perrin, C., & Michel, C. (2006). Dynamic averaging of rainfall‐runoff model simulations from complementary model parameterizations. Water Resources Research, 42(7). |\n",
    "| percent bias | a measure of the mean tendency of simulated values to be greater or less than associated observed values, units of percent |\n",
    "| ratio of standard deviation |standard deviation of simulated values divided by the standard deviation of observed values|\n",
    "| Pearson Correlation | K. Pearson (1896, 1900, 1920) |\n",
    "|Spearman Correlation | Charles Spearman (1904, 1910) |\n",
    "|percent bias in midsegment slope of the flow-duration curve (FDC) between Q20-Q70 | Yilmaz, K. K., Gupta, H. V., & Wagener, T. (2008). A process‐based diagnostic approach to model evaluation: Application to the NWS distributed hydrologic model. Water Resources Research, 44(9). |\n",
    "|percent bias in FDC low-segment volume (Q0-Q30)| Yilmaz, K. K., Gupta, H. V., & Wagener, T. (2008). A process‐based diagnostic approach to model evaluation: Application to the NWS distributed hydrologic model. Water Resources Research, 44(9). |\n",
    "|percent bias in FDC high-segment volume (Q98-Q100) | Yilmaz, K. K., Gupta, H. V., & Wagener, T. (2008). A process‐based diagnostic approach to model evaluation: Application to the NWS distributed hydrologic model. Water Resources Research, 44(9). | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10b4efb-5e1f-4734-a4bf-c4acce45d62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Global variance of observed data: dscore scorecard development\n",
    "'''\n",
    "def mse(obs):\n",
    "    \"\"\"\n",
    "    Calculate the global variance\n",
    "    \n",
    "    Args:\n",
    "        obs: numpy array of observed values\n",
    "    Returns:\n",
    "        variance\n",
    "    \"\"\"\n",
    "    return np.mean((obs - mod) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62327185-50a2-4fa6-bdd1-5a876c918001",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "A selection of traditional/standard statistical suite of metrics\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def mse(obs, mod):\n",
    "    \"\"\"\n",
    "    Calculate the mean squared error (MSE)\n",
    "    \n",
    "    Args:\n",
    "        obs: numpy array of observed values\n",
    "        mod: numpy array of modeled values\n",
    "    Returns:\n",
    "        mean squared error\n",
    "    \"\"\"\n",
    "    return np.mean((obs - mod) ** 2)\n",
    "\n",
    "\n",
    "def nse(obs, mod):\n",
    "    \"\"\"\n",
    "    Calculate the Nash-Sutcliffe Efficiency (NSE)\n",
    "    (https://www.sciencedirect.com/science/article/pii/0022169470902556?via%3Dihub)\n",
    "    \n",
    "    Args:\n",
    "        obs: numpy array of observed values\n",
    "        mod: numpy array of modeled values\n",
    "    Returns:\n",
    "        Nash-Sutcliffe Efficiency\n",
    "    \"\"\"\n",
    "    return 1 - (mse(obs, mod) / np.var(obs))\n",
    "\n",
    "\n",
    "def pbias(obs, mod):\n",
    "    \"\"\"\n",
    "    Calculate the percent bias\n",
    "    \n",
    "    Args:\n",
    "        obs: numpy array of observed values\n",
    "        mod: numpy array of modeled values\n",
    "    Returns:\n",
    "        Percent bias\n",
    "    \"\"\"\n",
    "    return 100 * ((np.sum(mod - obs)) / (np.sum(obs)))\n",
    "\n",
    "\n",
    "def pbias_percentile(obs, model, percentile, fun):\n",
    "    \"\"\"\n",
    "    Calculate the percent bias for a percentile bin\n",
    "    \n",
    "    Args:\n",
    "        obs: numpy array of observed values\n",
    "        mod: numpy array of modeled values\n",
    "        percentile: float\n",
    "        fun: comparison function (e.g., np.greater)\n",
    "    Returns:\n",
    "        Percent bias for bin\n",
    "    \"\"\"\n",
    "    threshold = np.percentile(obs, q=percentile)\n",
    "    i = fun(obs, threshold)\n",
    "    \n",
    "    return pbias(obs[i], model[i])\n",
    "    \n",
    "\n",
    "def pearson_r(obs, mod):\n",
    "    \"\"\"\n",
    "    Calculate Pearson's r\n",
    "    \n",
    "    Args:\n",
    "        obs: numpy array of observed values\n",
    "        mod: numpy array of modeled values\n",
    "    Returns:\n",
    "        Pearson's r\n",
    "    \"\"\"\n",
    "    #return np.cov(mod, obs) / np.sqrt( np.var(mod) * np.var(obs))\n",
    "    return np.corrcoef(mod, obs)[0,1]\n",
    "\n",
    "\n",
    "def spearman_r(obs, mod):\n",
    "    \"\"\"\n",
    "    Calculate Spearman's r\n",
    "    \n",
    "    Args:\n",
    "        obs: numpy array of observed values\n",
    "        mod: numpy array of modeled values\n",
    "    Returns:\n",
    "        Spearman's r\n",
    "    \"\"\"\n",
    "    return pearson_r(np.argsort(mod), np.argsort(obs))\n",
    "\n",
    "\n",
    "def kge(obs, mod):\n",
    "    \"\"\"\n",
    "    Calculate the Kling-Gupta Efficiency (KGE)\n",
    "    (https://www.sciencedirect.com/science/article/pii/S0022169409004843)\n",
    "    \n",
    "    Args:\n",
    "        obs: numpy array of observed values\n",
    "        mod: numpy array of modeled values\n",
    "    Returns:\n",
    "        Kling-Gupta Efficiency\n",
    "    \"\"\"\n",
    "    r = pearson_r(obs, mod)\n",
    "    alpha = sd_ratio(obs, mod)\n",
    "    beta = np.sum(mod) / np.sum(obs)\n",
    "    return 1 - np.sqrt((r-1)**2 + (alpha-1)**2 + (beta-1)**2)\n",
    "\n",
    "\n",
    "def sd_ratio(obs, mod):\n",
    "    \"\"\"\n",
    "    Calculate the standard deviation ratio of the model predictions and observations\n",
    "    \n",
    "    Args:\n",
    "        obs: numpy array of observed values\n",
    "        mod: numpy array of modeled values\n",
    "    Returns:\n",
    "        Standard deviation ratio   \n",
    "    \"\"\"\n",
    "    return np.std(mod) / np.std(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b833ef3-e284-48db-be43-eb5d96840ed9",
   "metadata": {},
   "source": [
    "### 1.3 Define benchmark function\n",
    "For each streamgage, we will compute a series of performance metrics, and the results from streamgage will be appended into a single dataframe, with one row per gage and one column for metric. To paralleize this task, we create the helper function `compute_benchmark()`, which computes the benchmark for a particular streamgage. In parallel, each worker in the cluster is assigned a gage, loads the data for that gage, computes the benchmark, and all the results are gathered. In this example, the `compute_benchmark()` function converts the data to a `pandas.Series`, resamples the model and observation data to the same timeseries, then computes a series of metrics from the resampled data. Each metric is stored as an entry in another `pandas.Series` named scores, which is returned by `compute_benchmark()` upon completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455a39bd-c0fe-417a-be73-10299e7bac4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_benchmark(gage_id, observations, predictions):\n",
    "    obs1 = observations.sel(gage_id=gage_id).load(scheduler='single-threaded').to_series()\n",
    "    mod1 = predictions.sel(gage_id=gage_id).load(scheduler='single-threaded').to_series().resample('1D', offset='5h').mean() # Resampling could be done in preanalysis\n",
    "    \n",
    "    # make sure the indices match\n",
    "    obs1.index = obs1.index.to_period('D')\n",
    "    mod1.index = mod1.index.to_period('D')\n",
    "\n",
    "    # merge obs and predictions and drop nans.\n",
    "    df = pd.merge(obs1, mod1, left_index=True, right_index=True).dropna(how='any')\n",
    "    obs1 = df['observed']\n",
    "    mod1 = df['predicted']\n",
    "    \n",
    "    # compute log flow for use in log NSE\n",
    "    threshold = 0.01\n",
    "    log_obs = np.log(obs1.where(obs1 > threshold, threshold))\n",
    "    log_model = np.log(mod1.where(mod1 > threshold, threshold))\n",
    "    \n",
    "    scores = pd.Series(dtype='float')\n",
    "    scores['nse'] = nse(obs1, mod1)\n",
    "    scores['log_nse'] = nse(log_obs, log_model)\n",
    "    scores['kge'] = kge(obs1, mod1)\n",
    "    \n",
    "    scores['pbias'] = pbias(obs1, mod1)\n",
    "    scores['pearson_r'] = pearson_r(obs1, mod1)\n",
    "    scores['spearman_r'] = spearman_r(obs1, mod1)\n",
    "    scores['sd_ratio'] = sd_ratio(obs1, mod1)\n",
    "    \n",
    "    # compute high flow and low flow bias (Yilmaz et al., 2008)\n",
    "    #   Yilmaz, K. K., Gupta, H. V., & Wagener, T. (2008). \n",
    "    #   A process‐based diagnostic approach to model evaluation: \n",
    "    #   Application to the NWS distributed hydrologic model. \n",
    "    #   Water Resources Research, 44(9).\n",
    "    high_percentile = 98\n",
    "    low_percentile = 30\n",
    "    \n",
    "    scores['pbias_q' + str(high_percentile)] = pbias_percentile(obs1, mod1, high_percentile, np.greater)\n",
    "    scores['pbias_q' + str(low_percentile)] = pbias_percentile(obs1, mod1, high_percentile, np.less_equal)\n",
    "    scores.name = gage_id\n",
    "    \n",
    "    #compute slope of the FDC curve (Yilmaz et al 2008)\n",
    "    #   Yilmaz, K. K., Gupta, H. V., & Wagener, T. (2008). \n",
    "    #   A process‐based diagnostic approach to model evaluation: \n",
    "    #   Application to the NWS distributed hydrologic model. \n",
    "    #   Water Resources Research, 44(9).\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c9c760-d1eb-45f6-b191-ca13452515dd",
   "metadata": {},
   "source": [
    "Run `compute_metrics()` and verify the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9654f30-6839-48e1-8062-5817868766d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod, obs = load_streamflow_data(resource)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc67d14-ada2-444a-a6e7-09d973a1656f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# run for a single site using 1 core\n",
    "gage_id = 'USGS-01030350'\n",
    "compute_benchmark(gage_id, obs, mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c10666-9dc3-44e7-b6e7-08e63a43545a",
   "metadata": {},
   "source": [
    "## 2. Compute benchmark results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b63c23c-2cd8-42c2-8125-595567b26e66",
   "metadata": {},
   "source": [
    "We will define one final function, that wraps `compute_benchmark()` in a `try` statement. That way, if an error occurs at a particular streamgage, the other streamgages will be unaffected. \n",
    "\n",
    "WARNING: While developing your code, we recommend against sequestering errors inside a `try`, because error messages are extremely useful when debugging code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d2e8df-dfab-4af6-873d-805c29dfe026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_compute_benchmark(gage_id):\n",
    "    \"\"\"Wrapper function\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return compute_benchmark(gage_id, obs, mod)\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906e5bd6-2065-4980-9bb2-4e899c94b820",
   "metadata": {},
   "source": [
    "### 2.0 Setup cluster\n",
    "Using our helper function, we can setup our analysis in two lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c53cc2-5369-44f5-b101-200e4a5c3b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "client, cluster = configure_cluster(resource)\n",
    "mod, obs = load_streamflow_data(resource)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d78ab3-8a4d-41bf-83c8-5bc08876824d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster.scale(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbfb795-f152-41db-b507-1672e428f4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d168f9-4087-4567-8995-6111cca0c9b0",
   "metadata": {},
   "source": [
    "### 2.1 Distribute with Dask bag - (applicable if on Cloud???)\n",
    "Now to parallelize, we create a Dask bag from the list `benchmark_gages`, and pass `try_compute_benchmark` to each element in the bag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0b820f-c1c0-4a95-a486-4e0ebdbc7b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = db.from_sequence(benchmark_gages, npartitions=40)\n",
    "b1 = b.map(try_compute_benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ca8d95-aeba-4592-8d9e-099b9034213f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#b = db.from_sequence(urls[:90], npartitions=3)\n",
    "#b1 = b.map(gen_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1dd87d-8020-493f-9479-40cd063da776",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from dask.distributed import performance_report\n",
    "with performance_report(filename=\"dask-report-whole.html\"):\n",
    "    results = b1.compute(retries=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e839c494-65ab-4ed7-8c68-e548d9e1cd22",
   "metadata": {},
   "source": [
    "Finally, concatenate the results from each gage into a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43087ac2-278c-4c14-8ba5-811b941f0fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [i for i in results if i is not None] # Drop entries where compute_metrics failed\n",
    "\n",
    "df_results = pd.concat(results, axis=1)\n",
    "df_results = df_results.T\n",
    "df_results.index.name = 'site_no'\n",
    "#df_results.index = df_results.index.astype('<U15')\n",
    "ds_results = df_results.to_xarray()\n",
    "ds_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad992e5-b1d6-4a78-8c9f-88f616124169",
   "metadata": {},
   "source": [
    "### 2.2 Save results to disk\n",
    "Save results to disk as a 'csv' file format, which can then be uploaded to USGS ScienceBase.\n",
    "This file will save to your HOME directory if you are working in the HPC environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6d6984-2026-4321-8944-9f416d929513",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_results.to_dataframe().to_csv('nwm_v2.1_streamflow_benchmark_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc15622-ef6e-41d1-8b26-f389b429f5dc",
   "metadata": {},
   "source": [
    "Then as a NetCDF, which we will later use for visualization. Let's add latitude and longitude from the benchmark data before writing the results to NetCDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e90005-11a6-4804-b828-0c6f96b360bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_results.merge(benchmark_ds, join='inner').to_netcdf('nwm_v2.1_streamflow_benchmark_test.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6862ab3e-c624-462f-87ac-83f6cbdfd82d",
   "metadata": {},
   "source": [
    "The end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b0e7ee-1b50-4110-a93c-7384b932c050",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close(); cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pangeo] *",
   "language": "python",
   "name": "conda-env-pangeo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
